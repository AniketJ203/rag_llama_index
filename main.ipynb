{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieval Augmented Generation\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='26d40bba-0160-4fc3-a7fe-83888bd2e65c', embedding=None, metadata={'page_label': '1', 'file_name': 'ROUGE.pdf', 'file_path': 'data\\\\ROUGE.pdf', 'file_type': 'application/pdf', 'file_size': 77520, 'creation_date': '2024-01-30', 'last_modified_date': '2023-12-30', 'last_accessed_date': '2024-01-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='ROUGE : A Package for Automatic Evaluation of Summaries  \\nChin -Yew Lin  \\nInformation Sciences Institute  \\nUniversity of Southern California  \\n4676 Admiralty Way  \\nMarina del Rey, CA  90292  \\ncyl@isi.edu  \\n \\nAbstract  \\nROUGE  stands for Recall -Oriented Unde rstudy for \\nGisting Evaluation. It includes  measures  to aut o-\\nmatically determine the quality of a summary by \\ncomparing it to other (ideal) summaries created by \\nhumans. The measure s count the number of ove r-\\nlapping  units such as n -gram, word sequences, and \\nword pairs  between the computer -generated su m-\\nmary to be evaluated and the ideal summaries cr e-\\nated by humans. This paper introduces four different \\nROUGE  measures: ROUGE -N, ROUGE -L, ROUGE -W, \\nand ROUGE -S included in the ROUGE  summariz a-\\ntion evalu ation package and their evaluatio ns. Three \\nof them have been used in the Document Unde r-\\nstanding Conference (DUC) 2004, a large -scale \\nsummar ization evaluation sponsored by NIST.  \\n1 Introduction  \\nTraditionally evaluation of summarization i nvolves \\nhuman judgment s of different quality metrics , for \\nexample, coherence, conciseness, grammaticality, \\nreadability, and content  (Mani , 2001) . However,  \\neven simple manual evaluation of summaries on a \\nlarge scale over a few lingui stic quality questions \\nand content coverage as in the Document Unde r-\\nstanding Con ference (DUC) (Over and Yen , 2003)  \\nwould require over 3 ,000 hours o f human e fforts. \\nThis is very ex pensive and difficult to co nduct in a \\nfrequent basis.  Therefore, how to evaluate summ a-\\nries automat ically has drawn a lot of attent ion in the \\nsummarization re search comm unity in recent years. \\nFor examp le, Sa ggion et al. (2002) proposed three  \\ncontent -based evaluation methods that mea sure \\nsimilarity between summ aries. These methods are:  \\ncosine similarity , unit overlap  (i.e. unigram or b i-\\ngram) , and longest common subsequence . However, \\nthey did not show how the r esults of these automatic \\nevaluation methods correlate to human judgments. \\nFollowing t he success ful applic ation of automatic \\nevaluation  method s, such as BLEU  (Papineni et al. , \\n2001), in machine translation e valuation, Lin and \\nHovy (2003 ) showed that methods similar to BLEU , i.e. n-gram  co-occurrence stati stics, could  be applied \\nto evalua te summaries.  In this paper, we introduce a \\npackage , ROUGE , for automatic evaluation of su m-\\nmaries  and its evaluation s. ROUGE  stands for R e-\\ncall-Oriented Understudy for Gisting Evaluation . It \\nincludes several automatic evalu ation methods  that \\nmeasure the similarity between summaries.  We d e-\\nscribe ROUGE -N in Section 2, ROUGE -L in Section \\n3, ROUGE -W in Section 4, and ROUGE -S in Sect ion \\n5. Section 6 show s how these measures correlate \\nwith human jud gments using DUC 2001, 2002, and \\n2003 data . Section 7 conclude s this paper and di s-\\ncusses future dire ctions. \\n2 ROUGE -N: N -gram Co -Occurrence St atistics  \\nFormally, ROUGE -N is an n -gram recall b etween a \\ncandidate summary and a set of reference summ a-\\nries. ROUGE -N is computed as follows:  \\n \\nROUGE -N \\n∑ ∑∑ ∑\\n∈ ∈∈ ∈\\n=\\n} {} {\\n) () (\\nSummaries ReferenceS S gramSummaries ReferemceS S grammatch\\nnnnn\\ngram Countgram Count\\n (1) \\n \\nWhere n stands for the length of the n -gram, \\ngram n, and Count match (gram n) is the maximum nu m-\\nber of n -grams co -occurring in a ca ndidate summary \\nand a set of reference summaries.  \\nIt is clear that ROUGE -N is a recall -related mea s-\\nure because the denominator of the equation is the \\ntotal sum of the number of n -grams occu rring at the \\nreference summary side. A closely related measure, \\nBLEU, used in automatic evalu ation of machine \\ntranslation, is a precision -based measure. BLEU \\nmeasures how well a candidate translation matches \\na set of reference translations by counting the pe r-\\ncentage of n -grams in the candidate translation ove r-\\nlapping wit h the refe rences. Please  see Papineni et \\nal. (2001) for d etails about BLEU . \\nNote that the number of n -grams in the denomin a-\\ntor of the ROUGE -N formula increases as we add \\nmore references. This is intuitive and reasonable \\nbecause there might exist multiple g ood summ aries. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b40c77dc-893f-40ac-aae0-cf4f196d7ff9', embedding=None, metadata={'page_label': '2', 'file_name': 'ROUGE.pdf', 'file_path': 'data\\\\ROUGE.pdf', 'file_type': 'application/pdf', 'file_size': 77520, 'creation_date': '2024-01-30', 'last_modified_date': '2023-12-30', 'last_accessed_date': '2024-01-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Every time we add a reference into the pool, we e x-\\npand the space of alternative summaries. By co n-\\ntrolling what types of references we add to the \\nreference pool, we can design evaluations that f ocus \\non diffe rent aspects of summarization. Also  note \\nthat the numerator sums over all reference summ a-\\nries. This effe ctively gives more weight to matching \\nn-grams occurring in multiple references. Therefore \\na cand idate summary that contains words shared by \\nmore references is favored by the ROUGE -N mea s-\\nure. This is again very intuitive and reasonable b e-\\ncause we normally prefer a candidate summary that \\nis more similar to consensus among reference su m-\\nmaries.  \\n2.1 Multiple References  \\nSo far, we only demonstrated how to compute \\nROUGE -N using a single reference. Wh en mult iple \\nreferences are used, we compute pairwise su mmary -\\nlevel ROUGE -N between a candidate su mmary s and \\nevery reference, ri, in the refe rence set. We then \\ntake the maximum of pairwise summary -level \\nROUGE -N scores as the final multiple refe rence \\nROUGE -N score. This can be written as fo llows:  \\n \\nROUGE -Nmulti  = argmax i ROUGE -N(ri,s)  \\n \\nThis procedure is also applied to computation of \\nROUGE -L (Section 3), ROUGE -W (Section 4) , and \\nROUGE -S (Section 5).  In the implementation, we use \\na Jackknifing procedure. Giv en M refe rences, we \\ncompute the best score over M sets of M -1 refe r-\\nences. The final ROUGE -N score is the average of \\nthe M  ROUGE -N scores using different M -1 refe r-\\nences.  The Jac kknifing procedure is adopted since \\nwe often need to compare system and human p er-\\nformance and the reference su mmaries are usually \\nthe only h uman summaries available. Using this \\nprocedure, we are able to estimate average human \\nperformance by avera ging M  ROUGE -N scores of \\none refe rence vs. the rest M -1 references. Although \\nthe Jackknif ing procedure is not necessary when we \\njust want to compute ROUGE  scores using mu ltiple \\nreferences, it is applied in all ROUGE  score comp u-\\ntations in the ROUGE  evaluation package.  \\nIn the next section, we describe a ROUGE  measure \\nbased on longest common subs equences b etween \\ntwo summaries.  \\n3 ROUGE -L: Longest Common Subs equence  \\nA sequence Z = [z1, z2, ..., z n] is a subsequence of \\nanother sequence X = [x1, x2, ..., x m], if there exists a \\nstrict increasing sequence [ i1, i2, ..., i k] of indices of \\nX such that for a ll j = 1, 2, ..., k , we have xij = zj  \\n(Cormen et al. , 1989). Given two s equences X and \\nY, the longest common subs equence (LCS) of X and Y is a common subsequence with maximum length. \\nLCS has been used in identifying cognate cand i-\\ndates during construction of N-best translation lex i-\\ncon from parallel text. Melamed (1995) used the \\nratio (LCSR) between the length of the LCS of two \\nwords and the length of the longer word of the two \\nwords to measure the cognateness between them. \\nHe used LCS as an a pproximate stri ng matching \\nalgorithm. Saggion et al. (2002) used normalized \\npairwise LCS to compare simila rity between two \\ntexts in aut omatic summarization evaluation.   \\n3.1 Sentence -Level LCS  \\nTo apply LCS in summarization evaluation, we \\nview a summary sentence as a sequence of words. \\nThe intuition is that the longer the LCS of two \\nsummary sentences is, the more similar the two \\nsumm aries are. We propose using LCS -based F -\\nmeasure to estimate the similarity b etween two \\nsummaries X of length m and Y of length  n, assu m-\\ning X is a r eference summary sentence and Y is a \\ncandidate summary sentence, as fo llows:  \\n \\nRlcs \\nmYX LCS ),(=       (2 ) \\nPlcs \\nnYX LCS ),(=       (3 ) \\nFlcs  \\nlcs lcslcs lcs\\nP RPR\\n22) 1(\\nbb\\n++= (4) \\n \\nWhere LCS(X,Y) is the length of a longest co m-\\nmon subsequence of X and Y, and ß = Plcs/Rlcs when \\n?Flcs/?Rlcs_=_?Flcs/?Plcs.  In DUC, ß is set to a very \\nbig number ( ? 8) . Therefore, only Rlcs is consi d-\\nered. We call the LCS -based F -measure, i.e. Equ a-\\ntion 4 , ROUGE -L. Notice that ROUGE -L is 1 when X \\n= Y; while ROUGE -L is zero when LCS(X,Y) = 0, i.e. \\nthere is nothing in common b etween X and Y. F-\\nmeasure or its equiv alents has been shown to have \\nmet several theoretical criteria in measuring acc u-\\nracy involving more than one factor (Van Rijsbe r-\\ngen, 1979). The composite fa ctors are LCS -based \\nrecall and pr ecision in this case. Melamed et al. \\n(2003) used unigram F -measure to estimate m achine \\ntranslation quality and showed that un igram F -\\nmeasure was as good as BLEU .  \\nOne advantage of using LCS is that it does not r e-\\nquire consecutive matches but i n-sequence matches \\nthat reflect sentence level word order as n -grams. \\nThe other advantage is that it automatically i ncludes \\nlongest in -sequence common n -grams, therefore no \\npredefined n -gram length is necessary.  \\nROUGE -L as defined in Equation 4 has the pr op-\\nerty that its value is less than or equal to the min i-\\nmum of unigram F -measure of X and Y. Unigram ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8514c6c9-2490-46e0-af6a-01597b4411cd', embedding=None, metadata={'page_label': '3', 'file_name': 'ROUGE.pdf', 'file_path': 'data\\\\ROUGE.pdf', 'file_type': 'application/pdf', 'file_size': 77520, 'creation_date': '2024-01-30', 'last_modified_date': '2023-12-30', 'last_accessed_date': '2024-01-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='recall reflects the proportion of words in X (refe r-\\nence summary sentence) that are also present in Y \\n(candidate summary sentence); while unigram pr e-\\ncision i s the proportion of words in Y that are also in \\nX. Unigram recall and precision count all co -\\noccurring words regardless their orders; while \\nROUGE -L counts only in -sequence co -occurrences.  \\nBy only awarding credit to in -sequence un igram \\nmatches, ROUGE -L als o captures sentence level \\nstructure in a natural way. Consider the fo llowing \\nexample:  \\n \\nS1. police killed the gunman  \\nS2. police  kill the gu nman \\nS3. the gunman  kill p olice \\n \\nWe only consider ROUGE -2, i.e. N=2, for the pu r-\\npose of explanation. Using S1 as the refe rence and  \\nS2 and S3 as the candidate summary se ntences , S2 \\nand S3 would have the same ROUGE -2 score, since \\nthey bot h have one bigram, i.e. “the gunman”.  How-\\never, S2 and S3 have very different meanings.  In the \\ncase of ROUGE -L, S2 has a score of 3/4 = 0.75 and \\nS3 has  a score of 2/4 = 0.5, with ß = 1. Ther efore S2 \\nis better than S3 according to ROUGE -L. This exa m-\\nple also illustrated that ROUGE -L can work reli ably \\nat sentence level.  \\nHowever, LCS suffers one disadvantage that it \\nonly counts the main in -sequence words; ther efore, \\nother alternative L CSes and shorter s equences are \\nnot reflected in the final score. For example, given \\nthe follo wing candidate sentence:  \\nS4. the gunman  police killed  \\nUsing S1 as its reference, LCS counts either “the \\ngunman” or “p olice killed”, but not both; therefore, \\nS4 has the  same ROUGE -L score as S3. ROUGE -2 \\nwould prefer S4 than S3.  \\n3.2 Summary -Level LCS  \\nPrevious section described how to compute se n-\\ntence -level LCS -based F -measure score. When a p-\\nplying to summary -level, we take the union LCS \\nmatches between a reference su mmary sent ence, ri, \\nand every candidate summary  sentence, cj. Given a \\nreference summary of u sentences containing a total \\nof m words and a candidate  summary of v sentences \\ncontaining a total of n word s, the su mmary -level \\nLCS-based F -measure can be computed as follow s: \\nRlcs mCr LCSu\\nii ∑=∪\\n=1),(\\n      (5)  \\nPlcs nCr LCSu\\nii ∑=∪\\n=1),(\\n      (6)  Flcs  \\nlcs lcslcs lcs\\nP RPR\\n22) 1(\\nbb\\n++=    (7)  \\n \\nAgain ß is set to a very big number ( ? 8)  in \\nDUC , i.e. only Rlcs is considered.  ),(Cr LCSi ∪is the \\nLCS score of the union  longest common subs e-\\nquence between reference sentence ri and cand idate \\nsummary C. For example, if ri = w1 w2 w3 w4 w5, and \\nC cont ains two sentences:  c1 = w1 w2 w6 w7 w8 and c2 \\n= w1 w3 w8 w9 w5, then the longest common subs e-\\nquence  of ri and c1 is “w1 w2” and the longest co m-\\nmon subsequence  of ri and c2 is “w1 w3 w5”. The \\nunion longest common subsequence  of ri, c1, and c2 \\nis “w1 w2 w3 w5” and ),(Cr LCSi ∪= 4/5.  \\n3.3 ROUGE -L vs. Normalized Pairwise LCS  \\nThe normalized pairwise LCS proposed by Radev et \\nal. (page 51, 2002) between two summ aries S1 and \\nS2, LCS(S 1 ,S2)MEAD , is written as fo llows:  \\n \\n∑ ∑∑ ∑\\n∈ ∈∈ ∈∈ ∈\\n++\\n1 21 21 2\\n)( )(),( max ),( max\\nSs S sj iSs S sj i Ss j i Ss\\ni ji ji j\\ns length s lengthss LCS ss LCS (8) \\n \\nAssuming S1 has m words and S2 has n words, \\nEquation 8 can be rewritten as Equation 9 due to \\nsymmetry:  \\n \\nnmss LCSSsj i Ssij\\n+∑∈∈12),( max *2\\n                       (9) \\n \\nWe then define MEAD LCS recall ( Rlcs-MEAD ) and \\nMEAD LCS precision ( Plcs-MEAD ) as follows:  \\n \\n Rlcs-MEAD  mss LCSSsj i Ssij∑∈∈\\n=12),( max\\n      (10)  \\nPlcs-MEAD  nss LCSSsj i Ssij∑∈∈\\n=12),( max\\n       (11) \\n \\nWe can rewrite Equation (9) in terms of Rlcs-MEAD  \\nand Plcs-MEAD  with a constant parameter ß = 1 as fo l-\\nlows:  \\nLCS(S 1 ,S2)MEAD  \\nMEAD lcs MEAD lcsMEADlcs MEADlcs\\nP RP R\\n− −− −\\n++=22) 1(\\nbb (12) \\nEquation 12 shows that normalized pairwise LCS \\nas defined in Radev et al. (2002) and impl emented \\nin MEAD is also a F -measure with ß = 1. Sentence -\\nlevel normalized pairwise LCS is the same as \\nROUGE -L with ß = 1. Besides setting ß = 1, su m-\\nmary -level normalized pairwise LCS is di fferent \\nfrom ROUGE -L in how a sentence gets its LCS score \\nfrom its references. Normalized pai rwise LCS takes ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='97eed791-cd83-499e-87cf-7223a056deee', embedding=None, metadata={'page_label': '4', 'file_name': 'ROUGE.pdf', 'file_path': 'data\\\\ROUGE.pdf', 'file_type': 'application/pdf', 'file_size': 77520, 'creation_date': '2024-01-30', 'last_modified_date': '2023-12-30', 'last_accessed_date': '2024-01-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='the best LCS score while ROUGE -L takes the union \\nLCS score.  \\n4 ROUGE -W: Weighted Longest Common Su b-\\nsequence  \\nLCS has many nice properties as we have d escribed \\nin the previous sections. Unfortunately, the basic \\nLCS also has a problem that it does not di fferentiate \\nLCSes of different spatial relations within their e m-\\nbeddin g sequences. For example, given a reference \\nsequence X and two candidate sequences Y1 and Y2 \\nas follows:  \\n \\nX:  [A B C D E F G]  \\nY1: [A B C D H I K]  \\nY2:  [A H B K C I D] \\n \\nY1 and Y2 have the same ROUGE -L score. Ho w-\\never, in this case, Y1 should be the better ch oice than \\nY2 because Y1 has consecutive matches. To improve \\nthe basic LCS method, we can simply r emember the \\nlength of consecutive matches encou ntered so far to \\na regular two dimensional dynamic program table \\ncomputing LCS. We call this weighted LCS \\n(WLCS)  and use k to indicate the length of the cu r-\\nrent consecutive matches ending at words xi and yj. \\nGiven two se ntences X and Y, the WLCS score of X \\nand Y can be computed using the following dynamic \\nprogramming procedur e: \\n \\n(1) For ( i = 0; i <=m; i++) \\n        c(i,j) = 0  // initialize c -table  \\n        w(i,j) = 0 // initialize w -table  \\n(2) For ( i = 1; i <= m; i++) \\n        For (j = 1; j <= n; j++) \\n          If xi = yj Then  \\n     // the length of consecutive matches at  \\n     // position i -1 and j -1 \\n     k = w(i-1,j-1) \\n     c(i,j) = c(i-1,j-1) + f(k+1) – f(k) \\n     // remember the length of consecutive  \\n     // matches at position i, j  \\n     w(i,j) = k+1 \\n          Otherwise  \\n     If c(i-1,j) > c(i,j-1) Then  \\n    c(i,j) = c(i-1,j) \\n    w(i,j) = 0           // no match at i , j \\n     Else c(i,j) = c(i,j-1) \\n     w(i,j) = 0           // no match at  i, j \\n(3) WLCS (X,Y) = c(m,n) \\n \\nWhere c is the dynamic programming table, c(i,j) \\nstores the WLCS score ending at word xi of X and yj \\nof Y, w is the table storing the length of consec utive \\nmatches ended at c table position i and j, and f is a \\nfunction of consecutive matches at the table pos i-tion, c(i,j). Notice that by providing di fferent \\nweighting function f, we can parameterize the \\nWLCS algorithm to assign different credit to co n-\\nsecutive in -sequence matches.  \\nThe weighting function f must have the pro perty \\nthat f(x+y) > f(x) + f(y) for any positive int egers x \\nand y. In other words, co nsecutive matches are \\nawarded more scores than non -consecutive matches. \\nFor e xample, f(k)-=-ak – b when  k >= 0,  and a, b > \\n0. This function charges a gap pe nalty of –b for \\neach non -consecutive n -gram sequences. Another \\npossible fun ction family is the polynomial family of \\nthe form ka where -a > 1. However, in order to \\nnorma lize the final ROUGE -W score, we also prefe r \\nto have a function that has a close form inverse \\nfunction. For example, f(k)-=-k2 has a close form \\ninverse function f -1(k)-=-k1/2. F-measure based on \\nWLCS can be computed as follows, given two s e-\\nquences X of length m and Y of length n: \\nRwlcs  \\uf8f7\\uf8f7\\n\\uf8f8\\uf8f6\\n\\uf8ec\\uf8ec\\n\\uf8ed\\uf8eb=−\\n)(),( 1\\nmfYX WLCSf       (13)  \\nPwlcs  \\uf8f7\\uf8f7\\n\\uf8f8\\uf8f6\\n\\uf8ec\\uf8ec\\n\\uf8ed\\uf8eb=−\\n)(),( 1\\nnfYX WLCSf       (14)  \\nFwlcs  \\nwlcs wlcswlcs wlcs\\nP RPR\\n22) 1(\\nbb\\n++=           (15)  \\n \\nWhere f -1 is the inverse function of f. In DUC, ß is \\nset to a very big number ( ? 8) . Ther efore, only \\nRwlcs  is co nsidered. We call the WLCS -based F -\\nmeasure, i.e. Equation 15, ROUGE -W. Using Equ a-\\ntion 15 and f(k)-=-k2 as the weighting fun ction, the \\nROUGE -W scores for s equences Y1 and Y2 are 0.571 \\nand 0.286  respe ctively. Therefore, Y1 would be \\nranked higher than Y2 using WLCS. We use the \\npolynomial fun ction of the form ka in the ROUGE  \\nevaluation package. In the next section, we intr o-\\nduce the skip -bigram co -occurrence stati stics. \\n5 ROUGE -S: Skip -Bigram Co -Occur rence St a-\\ntistics  \\nSkip-bigram is any pair of words in their se ntence \\norder, allowing for arbitrary gaps. Skip -bigram co -\\noccurrence statistics measure the ove rlap of skip -\\nbigrams between a candidate transl ation and a set of \\nreference translations. Using the example given in \\nSection 3.1:  \\n \\nS1. police killed the gu nman \\nS2. police kill the gu nman \\nS3. the gunman kill p olice \\nS4. the gunman police killed  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d891649a-b5b2-4f0d-b96d-2ed5159d7415', embedding=None, metadata={'page_label': '5', 'file_name': 'ROUGE.pdf', 'file_path': 'data\\\\ROUGE.pdf', 'file_type': 'application/pdf', 'file_size': 77520, 'creation_date': '2024-01-30', 'last_modified_date': '2023-12-30', 'last_accessed_date': '2024-01-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='each sentence has C(4,2)1 = 6 skip -bigrams. For e x-\\nample, S1 has the following skip -bigrams:  \\n(“police killed ”, “police the ”, “police gunman ”, \\n“killed the ”, “killed gunman ”, “the gu nman”)  \\nS2 has three skip -bigram matches with S1 (“ po-\\nlice the ”, “police gunman ”, “the gunman ”), S3 has \\none skip -bigram match with S1 (“ the gu nman”), and \\nS4 has two skip -bigram matches with S1 (“ police \\nkilled”, “the gunman ”).  Given tran slations X of \\nlength m and Y of length  n, assuming X is a refe r-\\nence translation and Y is a candidate translation, we \\ncompute skip -bigram -based F -measure as fo llows:  \\nRskip2  \\n)2,(),(2\\nmCYX SKIP=           (16)  \\nPskip2  \\n)2,(),(2\\nnCYX SKIP=           (17)  \\nFskip2  \\n22\\n22 22\\n) 1(\\nskip skipskip skip\\nP RP R\\nbb\\n++=   (18)  \\n \\nWhere SKIP2 (X,Y) is the number of skip -bigram \\nmatches between X and Y, ß controlling the relative \\nimportance of  Pskip2  and Rskip2 , and  C is the comb i-\\nnation fun ction. We call the skip -bigram -based F -\\nmeasure, i.e. Equ ation 18, ROUGE -S. \\nUsing Equation 18 with ß = 1 and S1 as the ref er-\\nence, S2’s ROUGE -S score is 0.5, S3 is 0.167 , and \\nS4 is 0.333. Therefore, S2 is better than S3 and S4, \\nand S4 is better than S3. This result is more intu itive \\nthan using BLEU -2 and ROUGE -L. One adva ntage of \\nskip-bigram vs. BLEU  is that it does not require co n-\\nsecutive matches but is still sens itive t o word order. \\nComparing skip -bigram with LCS, skip -bigram \\ncounts all in -order matching word pairs while LCS \\nonly counts one longest common subs equence.  \\nApplying skip -bigram without any constraint on \\nthe distance between the words, spurious matches \\nsuch as “the the ” or “ of in ” might be counted as \\nvalid matches. To reduce these spur ious matches, \\nwe can limit the maximum skip distance, dskip, be-\\ntween two in -order words that is allowed to form a \\nskip-bigram. For exa mple, if we set dskip to 0 then \\nROUGE -S is equ ivalent to bigram overlap F -\\nmeasure. If we set dskip to 4 then only word pairs of \\nat most 4 words apart can form skip -bigrams.  \\nAdjusting Equations 16, 17, and 18 to use max i-\\nmum skip distance limit is straightforward: we only \\ncount the skip -bigram matches, SKIP2 (X,Y), within \\nthe maximum skip distance and replace denomin a-\\ntors of Equations 16, C(m,2), and 17, C(n,2), with \\nthe actual numbers of within distance skip -bigrams \\nfrom the reference and the candidate respe ctively.  \\n \\n                                                                 \\n1 C(4,2) = 4!/(2!*2!) = 6.  5.1 ROUGE -SU: Extension  of ROUGE -S \\nOne po tential problem for ROUGE -S is that it does \\nnot give any credit to a candidate sentence if the \\nsentence does not have any word pair co -occurring \\nwith its references.  For example, the following se n-\\ntence  has a ROUGE -S score of zero : \\n \\nS5. gunman the killed police  \\n \\nS5 is the exact reverse of S1 and there is no skip \\nbigram match between them.  However, we would \\nlike to differentiate sentences similar to S5 from \\nsentences that do not have single word co-\\noccurrence with S1.  To achieve this, we extend \\nROUGE -S with the addition of  unigram as counting \\nunit. The extended version is called ROUGE -SU. We \\ncan also obtain  ROUGE -SU from ROUGE -S by add-\\ning a b egin-of-sentence marker at the beginning  of \\ncandidate and reference  sentences.  \\n6 Evaluations of ROUGE  \\nTo assess the effectiven ess of ROUGE  measures, we \\ncomp ute the correlation between ROUGE  assigned \\nsummary scores and human assigned su mmary \\nscores.  The intuition is that a good evaluation mea s-\\nure should assign a good score to a good summary  \\nand a bad score  to a bad su mmary. The gr ound truth \\nis based on human assigned scores.  Acquiring h u-\\nman judgments are usually very expensive; fort u-\\nnately, w e have DUC 2001, 2002, and 2003 \\nevaluation data  that include  human judgment s for \\nthe fo llowing : \\n• Single  document summaries of about 100 \\nwords : 12 systems 2 for DUC 2001 and 14 sy s-\\ntems for 2002. 149 single docu ment summaries \\nwere judged  per system in  DUC  2001 and 295 \\nwere judged in DUC 2002 . \\n• Single document very short summaries of about \\n10 words (headline -like, keywords, or phrases) : \\n14 systems for  DUC 2003 . 624 very short su m-\\nmaries were judged per system in DUC 2003.  \\n• Multi -document summaries of about 10 words: \\n6 systems for DUC 2002 ; 50 words: 14 sy stems \\nfor DUC 2001 and 10 systems for DUC 2002; \\n100 words: 14 systems for DUC 2001, 10 sys-\\ntems for DU C 2002, and 18 systems for DUC \\n2003; 200 words: 14 systems for DUC 2001 and \\n10 systems for DUC 2002 ; 400 words: 14 sy s-\\ntems for  DUC 2001 . 29 summ aries were judged \\nper system per summary size in DUC 2001, 59 \\nwere judged in DUC 2002, and 30 were judged \\nin DUC  2003.  \\n                                                                 \\n2 All systems includ e 1 or 2 baselines. Please see DUC \\nwebsite for details.  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='815db90a-1cd0-4635-b956-978654d33969', embedding=None, metadata={'page_label': '6', 'file_name': 'ROUGE.pdf', 'file_path': 'data\\\\ROUGE.pdf', 'file_type': 'application/pdf', 'file_size': 77520, 'creation_date': '2024-01-30', 'last_modified_date': '2023-12-30', 'last_accessed_date': '2024-01-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Besides these human judgments, we also have 3 sets \\nof manual summaries for DUC 2001, 2 sets for \\nDUC 2002, and 4 sets for DUC 2003.  Human \\njudges assigned content coverage scores to a cand i-\\ndate summary by examining the percentage of co n-\\ntent overlap be tween a manual summary unit, i.e. \\nelementary discourse unit or sentence, and the ca n-\\ndidat e summary using Summary Evaluation Env i-\\nronment 3 (SEE) developed by  the Un iversity of \\nSouthern California’s Information Sciences I nstitute  \\n(ISI). The overall candidate  summary score is the \\naverage of the content co verage score s of all the \\nunits in the  manual su mmary.  Note that human \\njudges used only one manual summary in all the \\nevaluations although multiple alternative summ aries \\nwere available.  \\nWith the DUC data, we computed Pearson’s \\nproduct moment correlation coefficient s, Spea r-\\nman’s rank order correlation coefficient s, and \\nKendall’s correlation coefficient s between systems’ \\naverage  ROUGE  scores an d their human a ssigned \\naverage coverage scores using single reference  and \\nmultiple references.  To investigate the effect of \\nstemming and inclusion or exclusion of stopwords, \\nwe also ran experiments over orig inal automatic  and \\n                                                                 \\n3 SEE is available online at http://www.isi.edu/~cyl.  manual summaries (CASE  set), stemmed 4 version of \\nthe summaries (STEM  set), and stopped  version of \\nthe summaries (STOP set). For example, we co m-\\nputed ROUGE  scores for the 12 sy stems participated \\nin the DUC 2001 single document summarization \\nevaluation using the CASE set with single refe rence \\nand then calculated the three correl ation scores for \\nthese 12 s ystems’ ROUGE  scores vs. human assigned \\naverage cove rage scores.  After that w e repeated the \\nprocess us ing multiple refe rences and then using \\nSTEM and STOP sets.  Therefore, 2 (multi or single) \\nx 3 (CASE, STEM, or STOP) x 3 (Pearson, Spea r-\\nman, or Kendall) = 18 data points were c ollected for \\neach ROUGE  measure and each DUC task.  To assess \\nthe significance of the results, we applied bootstrap \\nresampling technique (Davison and Hinkley , 1997) \\nto estimate 95% confidence intervals for every co r-\\nrelation computation . \\n17 ROUGE  measures were tested for each run  us-\\ning ROUGE  evaluation package v1.2.1 : ROUGE -N  \\nwith N = 1 to 9, R OUGE-L, ROUGE -W with \\nweighting factor a  = 1.2, ROUGE -S and ROUGE -SU \\nwith max imum skip distance  dskip = 1, 4, and 9.  Due \\nto limitation of space, we  only report correlation \\nanalysis results based on Pearson’s co rrelation coe f-\\nficient.  Correlation analyses based on Spea rman’s \\nand Kendall’s corr elation coefficients are tracking \\nPearson’s very closely and will be posted later at the \\nROUGE  website 5 for ref erence.  The critical value 6 \\nfor Pearson’s correl ation is 0.632  at 95% confidence  \\nwith 8 degrees of fre edom.  \\nTable 1 shows the Pearson’s correlation coeff i-\\ncients of the 17 ROUGE  measures vs. human judg-\\nments on DUC 2001 and 2002 100 word s single \\ndocument sum marization data . The best values in \\neach column are marked with dark  (green) color and \\nstatistically equivalent values to the best va lues are \\nmark ed with gray.  We found that correl ations were \\nnot affected by stemming or removal of stopwords  \\nin this data se t, ROUGE -2 performed be tter among \\nthe ROUGE -N variants, ROUGE -L, ROUGE -W, and \\nROUGE -S were all performing well, and using mu l-\\ntiple references improved pe rformance though not \\nmuch.  All ROUGE  measures achieved very good \\ncorrelation with human jud gments in th e DUC 2002 \\ndata. This might due to the double sa mple size in \\nDUC 2002 (295 vs. 149 in DUC 2001) for each sy s-\\ntem. \\nTable 2  shows the cor relation analysis results on \\nthe DUC 2003 single document very short su mmary  \\ndata.  We found that ROUGE -1, ROUGE -L, ROUGE -\\n                                                                 \\n4 Porter’s stemmer was used.  \\n5 ROUGE  website: http://www.isi.edu/~cyl/ROUGE.  \\n6 The critical  values for Pearson’s correlation at 95% \\nconfidence wit h 10, 12, 14, and 16 degrees of freedom \\nare 0.576, 0.532, 0.497, and 0.468 respe ctively.  Method CASE STEM STOP CASE STEM STOP CASE STEM STOP CASE STEM STOP\\nR-1 0.76 0.76 0.84 0.80 0.78 0.84 0.98 0.98 0.99 0.98 0.98 0.99\\nR-2 0.84 0.84 0.83 0.87 0.87 0.86 0.99 0.99 0.99 0.99 0.99 0.99\\nR-3 0.82 0.83 0.80 0.86 0.86 0.85 0.99 0.99 0.99 0.99 0.99 0.99\\nR-4 0.81 0.81 0.77 0.84 0.84 0.83 0.99 0.99 0.98 0.99 0.99 0.99\\nR-5 0.79 0.79 0.75 0.83 0.83 0.81 0.99 0.99 0.98 0.99 0.99 0.98\\nR-6 0.76 0.77 0.71 0.81 0.81 0.79 0.98 0.99 0.97 0.99 0.99 0.98\\nR-7 0.73 0.74 0.65 0.79 0.80 0.76 0.98 0.98 0.97 0.99 0.99 0.97\\nR-8 0.69 0.71 0.61 0.78 0.78 0.72 0.98 0.98 0.96 0.99 0.99 0.97\\nR-9 0.65 0.67 0.59 0.76 0.76 0.69 0.97 0.97 0.95 0.98 0.98 0.96\\nR-L 0.83 0.83 0.83 0.86 0.86 0.86 0.99 0.99 0.99 0.99 0.99 0.99\\nR-S* 0.74 0.74 0.80 0.78 0.77 0.82 0.98 0.98 0.98 0.98 0.97 0.98\\nR-S4 0.84 0.85 0.84 0.87 0.88 0.87 0.99 0.99 0.99 0.99 0.99 0.99\\nR-S9 0.84 0.85 0.84 0.87 0.88 0.87 0.99 0.99 0.99 0.99 0.99 0.99\\nR-SU* 0.74 0.74 0.81 0.78 0.77 0.83 0.98 0.98 0.98 0.98 0.98 0.98\\nR-SU4 0.84 0.84 0.85 0.87 0.87 0.87 0.99 0.99 0.99 0.99 0.99 0.99\\nR-SU9 0.84 0.84 0.85 0.87 0.87 0.87 0.99 0.99 0.99 0.99 0.99 0.99\\nR-W-1.2 0.85 0.85 0.85 0.87 0.87 0.87 0.99 0.99 0.99 0.99 0.99 0.99DUC 2001 100 WORDS SINGLE DOC DUC 2002 100 WORDS SINGLE DOC\\n1 REF 3 REFS 1 REF 2 REFS\\nTable  1: Pearson’s correlations of 17 ROUGE\\nmeasure scores vs. human judgments for the DUC \\n2001 and 2002 100 words single documen t sum-\\nmariz ation tasks  \\n1 REF 4REFS 1 REF 4 REFS 1 REF 4 REFS\\nMethod\\nR-1 0.96 0.95 0.95 0.95 0.90 0.90\\nR-2 0.75 0.76 0.75 0.75 0.76 0.77\\nR-3 0.71 0.70 0.70 0.68 0.73 0.70\\nR-4 0.64 0.65 0.62 0.63 0.69 0.66\\nR-5 0.62 0.64 0.60 0.63 0.63 0.60\\nR-6 0.57 0.62 0.55 0.61 0.46 0.54\\nR-7 0.56 0.56 0.58 0.60 0.46 0.44\\nR-8 0.55 0.53 0.54 0.55 0.00 0.24\\nR-9 0.51 0.47 0.51 0.49 0.00 0.14\\nR-L 0.97 0.96 0.97 0.96 0.97 0.96\\nR-S* 0.89 0.87 0.88 0.85 0.95 0.92\\nR-S4 0.88 0.89 0.88 0.88 0.95 0.96\\nR-S9 0.92 0.92 0.92 0.91 0.97 0.95\\nR-SU* 0.93 0.90 0.91 0.89 0.96 0.94\\nR-SU4 0.97 0.96 0.96 0.95 0.98 0.97\\nR-SU9 0.97 0.95 0.96 0.94 0.97 0.95\\nR-W-1.2 0.96 0.96 0.96 0.96 0.96 0.96DUC 2003 10 WORDS SINGLE DOC\\nCASE STEM STOP\\nTable 2 : Pearson’s correlations of 17 ROUGE\\nmeasure scores vs. human judgments for the D UC \\n2003 very short summary task  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4f7891c4-1abf-4509-986f-70b171e5fe72', embedding=None, metadata={'page_label': '7', 'file_name': 'ROUGE.pdf', 'file_path': 'data\\\\ROUGE.pdf', 'file_type': 'application/pdf', 'file_size': 77520, 'creation_date': '2024-01-30', 'last_modified_date': '2023-12-30', 'last_accessed_date': '2024-01-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='SU4 and 9, and ROUGE -W were very good measures  \\nin this cat egory, ROUGE -N with N > 1 performed \\nsignificantly worse than all other measures, and e x-\\nclusion of stopwords improved performance in ge n-\\neral except for ROUGE -1. Due to the large number \\nof samples (624 ) in this data set, using multiple re f-\\nerences did not improve correl ations.  \\nIn Table 3 A1, A2, and A3, we show correlation \\nanalysis results on DUC 2001, 2002, and 2003 100 \\nwords multi -document summarization data.  The \\nresults indicated that using multiple r eferences i m-\\nproved correlation and exclusion of stopwords us u-\\nally improved performance. ROUGE -1, 2, and 3 \\nperformed fine but were not consistent.  ROUGE -1, \\nROUGE -S4, ROUGE -SU4, ROUGE -S9, and ROUGE -\\nSU9 with stopword r emoval ha d correlation above \\n0.70. ROUGE -L and ROUGE -W did not work well in \\nthis set of data.  \\nTable 3 C, D1, D2, E1, E2, and F show  the corr e-\\nlation analyses using  multiple refe rences on the rest \\nof DUC data.  These results again suggested that \\nexclusion of stopwords achieved better pe rformance \\nespecially in multi -document summaries of 50 \\nwords.  Better correlations (> 0.70) were o bserved \\non long summary tasks, i.e. 200 and 400 words \\nsummaries.  The relative performance of ROUGE  \\nmeasures followed the pa ttern of the 100 words \\nmulti -document summarizat ion task.  \\nComparing the results in Table 3 with Table s 1 \\nand 2, we found that correlation values in the multi -\\ndocument tasks rarely reached high 90% e xcept in \\nlong summary tasks.  One possible explan ation of \\nthis outcome is that we did not have large  amoun t of \\nsamples for the multi -document task s. In the single \\ndocument summariz ation tasks we had over 100 samples; while we only had about 30 sam ples in the \\nmulti -document tasks.  The only task s that had over \\n30 samples was from DUC 2002 and the correl a-\\ntions of  ROUGE  measures with human judgments on \\nthe 100 words summary task were much better and \\nmore stable than similar tasks in DUC 2001 and \\n2003.  Statistically stable human judgments of sy s-\\ntem pe rformance might not be obtained due to lack \\nof samples  and this in  turn caused instability  of co r-\\nrelation analyses.  \\n7 Conclusions  \\nIn this paper , we introduced ROUGE , an automatic \\nevaluatio n package for summarization, and co n-\\nducted comprehensive evaluations of the automatic \\nmeasures included in the ROUGE  package using \\nthree  years of DUC data.  To check the significance \\nof the results, we estimated co nfidence intervals of \\ncorrelations using bootstrap resampling. We found \\nthat (1) ROUGE -2, ROUGE -L, ROUGE -W, and \\nROUGE -S worked well in single document summ a-\\nrization tasks,  (2) ROUGE-1, ROUGE -L, ROUGE -W, \\nROUGE -SU4, and ROUGE -SU9 performed great in \\nevalua ting very short summaries (or headline -like \\nsumm aries), (3) correlation of high 90% was hard to \\nachieve for multi -document summarization task s but \\nROUGE -1, ROUGE -2, ROUGE -S4, ROUGE -S9, \\nROUGE -SU4, and ROUGE -SU9 worked reason ably \\nwell when stopwords were exc luded from matc hing, \\n(4) exclusion of  stopwords usually improved corr e-\\nlation, and (5) correlations to human jud gments \\nwere increased by using multiple references.  \\nIn summary, we sho wed that the ROUGE  package \\ncould be used effectively in automatic evalu ation of \\nsummaries.  In a separate study (Lin and Och , 2004), Method CASE STEM STOP CASE STEM STOP CASE STEM STOP CASE STEM STOP CASE STEM STOP CASE STEM STOP\\nR-1 0.48 0.56 0.86 0.53 0.57 0.87 0.66 0.66 0.77 0.71 0.71 0.78 0.58 0.57 0.71 0.58 0.57 0.71\\nR-2 0.55 0.57 0.64 0.59 0.61 0.71 0.83 0.83 0.80 0.88 0.87 0.85 0.69 0.67 0.71 0.79 0.79 0.81\\nR-3 0.46 0.45 0.47 0.53 0.53 0.55 0.85 0.84 0.76 0.89 0.88 0.83 0.54 0.51 0.48 0.76 0.75 0.74\\nR-4 0.39 0.39 0.43 0.48 0.49 0.47 0.80 0.80 0.63 0.83 0.82 0.75 0.37 0.36 0.36 0.62 0.61 0.52\\nR-5 0.38 0.39 0.33 0.47 0.48 0.43 0.73 0.73 0.45 0.73 0.73 0.62 0.25 0.25 0.27 0.45 0.44 0.38\\nR-6 0.39 0.39 0.20 0.45 0.46 0.39 0.71 0.72 0.38 0.66 0.64 0.46 0.21 0.21 0.26 0.34 0.31 0.29\\nR-7 0.31 0.31 0.17 0.44 0.44 0.36 0.63 0.65 0.33 0.56 0.53 0.44 0.20 0.20 0.23 0.29 0.27 0.25\\nR-8 0.18 0.19 0.09 0.40 0.40 0.31 0.55 0.55 0.52 0.50 0.46 0.52 0.18 0.18 0.21 0.23 0.22 0.23\\nR-9 0.11 0.12 0.06 0.38 0.38 0.28 0.54 0.54 0.52 0.45 0.42 0.52 0.16 0.16 0.19 0.21 0.21 0.21\\nR-L 0.49 0.49 0.49 0.56 0.56 0.56 0.62 0.62 0.62 0.65 0.65 0.65 0.50 0.50 0.50 0.53 0.53 0.53\\nR-S* 0.45 0.52 0.84 0.51 0.54 0.86 0.69 0.69 0.77 0.73 0.73 0.79 0.60 0.60 0.67 0.61 0.60 0.70\\nR-S4 0.46 0.50 0.71 0.54 0.57 0.78 0.79 0.80 0.79 0.84 0.85 0.82 0.63 0.64 0.70 0.73 0.73 0.78\\nR-S9 0.42 0.49 0.77 0.53 0.56 0.81 0.79 0.80 0.78 0.83 0.84 0.81 0.65 0.65 0.70 0.70 0.70 0.76\\nR-SU* 0.45 0.52 0.84 0.51 0.54 0.87 0.69 0.69 0.77 0.73 0.73 0.79 0.60 0.59 0.67 0.60 0.60 0.70\\nR-SU4 0.47 0.53 0.80 0.55 0.58 0.83 0.76 0.76 0.79 0.80 0.81 0.81 0.64 0.64 0.74 0.68 0.68 0.76\\nR-SU9 0.44 0.50 0.80 0.53 0.57 0.84 0.77 0.78 0.78 0.81 0.82 0.81 0.65 0.65 0.72 0.68 0.68 0.75\\nR-W-1.2 0.52 0.52 0.52 0.60 0.60 0.60 0.67 0.67 0.67 0.69 0.69 0.69 0.53 0.53 0.53 0.58 0.58 0.58\\nMethod CASE STEM STOP CASE STEM STOP CASE STEM STOP CASE STEM STOP CASE STEM STOP CASE STEM STOP\\nR-1 0.71 0.68 0.49 0.49 0.49 0.73 0.44 0.48 0.80 0.81 0.81 0.90 0.84 0.84 0.91 0.74 0.73 0.90\\nR-2 0.82 0.85 0.80 0.43 0.45 0.59 0.47 0.49 0.62 0.84 0.85 0.86 0.93 0.93 0.94 0.88 0.88 0.87\\nR-3 0.59 0.74 0.75 0.32 0.33 0.39 0.36 0.36 0.45 0.80 0.80 0.81 0.90 0.91 0.91 0.84 0.84 0.82\\nR-4 0.25 0.36 0.16 0.28 0.26 0.36 0.28 0.28 0.39 0.77 0.78 0.78 0.87 0.88 0.88 0.80 0.80 0.75\\nR-5 -0.25 -0.25 -0.24 0.30 0.29 0.31 0.28 0.30 0.49 0.77 0.76 0.72 0.82 0.83 0.84 0.77 0.77 0.70\\nR-6 0.00 0.00 0.00 0.22 0.23 0.41 0.18 0.21 -0.17 0.75 0.75 0.67 0.78 0.79 0.77 0.74 0.74 0.63\\nR-7 0.00 0.00 0.00 0.26 0.23 0.50 0.11 0.16 0.00 0.72 0.72 0.62 0.72 0.73 0.74 0.70 0.70 0.58\\nR-8 0.00 0.00 0.00 0.32 0.32 0.34 -0.11 -0.11 0.00 0.68 0.68 0.54 0.71 0.71 0.70 0.66 0.66 0.52\\nR-9 0.00 0.00 0.00 0.30 0.30 0.34 -0.14 -0.14 0.00 0.64 0.64 0.48 0.70 0.69 0.59 0.63 0.62 0.46\\nR-L 0.78 0.78 0.78 0.56 0.56 0.56 0.50 0.50 0.50 0.81 0.81 0.81 0.88 0.88 0.88 0.82 0.82 0.82\\nR-S* 0.83 0.82 0.69 0.46 0.45 0.74 0.46 0.49 0.80 0.80 0.80 0.90 0.84 0.85 0.93 0.75 0.74 0.89\\nR-S4 0.85 0.86 0.76 0.40 0.41 0.69 0.42 0.44 0.73 0.82 0.82 0.87 0.91 0.91 0.93 0.85 0.85 0.85\\nR-S9 0.82 0.81 0.69 0.42 0.41 0.72 0.40 0.43 0.78 0.81 0.82 0.86 0.90 0.90 0.92 0.83 0.83 0.84\\nR-SU* 0.75 0.74 0.56 0.46 0.46 0.74 0.46 0.49 0.80 0.80 0.80 0.90 0.84 0.85 0.93 0.75 0.74 0.89\\nR-SU4 0.76 0.75 0.58 0.45 0.45 0.72 0.44 0.46 0.78 0.82 0.83 0.89 0.90 0.90 0.93 0.84 0.84 0.88\\nR-SU9 0.74 0.73 0.56 0.44 0.44 0.73 0.41 0.45 0.79 0.82 0.82 0.88 0.89 0.89 0.92 0.83 0.82 0.87\\nR-W-1.2 0.78 0.78 0.78 0.56 0.56 0.56 0.51 0.51 0.51 0.84 0.84 0.84 0.90 0.90 0.90 0.86 0.86 0.86(A1) DUC 2001 100 WORDS MULTI (A2) DUC 2002 100 WORDS MULTI (A3) DUC 2003 100 WORDS MULTI\\n1 RFF 3 REFS 1 REF 2 REFS 1 REF 4 REFS\\n(E2) DUC02 200 (F) DUC01 400 (C) DUC02 10 (D1) DUC01 50 (D2) DUC02 50 (E1) DUC01 200\\nTable 3:  Pearson’s correla tions of 17 ROUGE  measure scores vs. human judgments for \\nthe DUC 2001, 2002, and 2003 mul ti-document summarization tasks  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2e33fa73-d60f-4c4b-8e8f-defdae34e8aa', embedding=None, metadata={'page_label': '8', 'file_name': 'ROUGE.pdf', 'file_path': 'data\\\\ROUGE.pdf', 'file_type': 'application/pdf', 'file_size': 77520, 'creation_date': '2024-01-30', 'last_modified_date': '2023-12-30', 'last_accessed_date': '2024-01-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='ROUGE -L, W, and S were also shown to be very \\neffective  in automatic  evaluation of machine \\ntranslation. The stability and rel iability of ROUGE  at \\ndifferent sample sizes was reported by the author in \\n(Lin, 2004). However, how to achieve high correl a-\\ntion with human judgments in multi -document \\nsummarization tasks as ROUGE  already did in single \\ndocument summarization tasks is still an open r e-\\nsearch topic.  \\n8  Acknowledgements  \\nThe author would like to thank the anonymous r e-\\nviewers  for their constructive comments, Paul Over \\nat NIST , U.S.A , and ROUGE  users around the world  \\nfor testing and providing useful feedback on ea rlier \\nversion s of the  ROUGE  evaluation package, and the \\nDARPA TIDES project for supporting this r esearch.  \\nReferences  \\n \\nCormen, T. R., C. E. Leiserson, and R. L. Rivest. \\n1989. Introduction to Algorithms . The MIT Press.  \\nDavison, A. C. and D. V. Hinkley. 1997. Bootstrap \\nMethods an d Their Application . Cambridge Un i-\\nversity Press.  \\nLin, C. -Y. and E.  H. Hovy . 2003. Automatic e valua-\\ntion of summaries u sing n-gram co-occurrence \\nstatistics. In Proceedings of 2003 Language \\nTechnology Confe rence (HLT -NAACL 2003), \\nEdmonton, Ca nada. \\nLin, C. -Y. 2004. Looking for a f ew good metrics: \\nROUGE  and its evaluation. In Proceedings of \\nNTCIR Workshop 2004 , Tokyo, Japan.  \\nLin, C. -Y. and F.  J. Och. 2004.  Automatic evalua-\\ntion of machine t ranslation quality using longest \\ncommon subsequence and skip-bigram s tatistics. \\nIn Procee dings of 42nd Annual Meeting of ACL  \\n(ACL 200 4), Barcelona , Spain. \\nMani, I. 200 1. Automatic Summarization . John Be n-\\njamins Pu blishing Co.  \\nMelamed, I. D.  1995. Automatic evaluation and un i-\\nform f ilter cascades for inducing n-best transla-\\ntion lexicons. In Proceedings of the 3rd Workshop \\non Very Large Corpora (WVLC3) . Boston, \\nU.S.A.  \\nMelamed, I. D., R. Green and J. P. Turian (2003). \\nPrecision and recall of machine t ranslation . In \\nProcee dings of 2003 Language Technology Co n-\\nference  (HLT -NAA CL 2003), Edmonton, Ca n-\\nada. \\nOver, P. and J. Yen. 2003. An i ntrod uction to DUC \\n2003 – Intrinsic e valuation of generic news text \\nsummariz ation s ystems.  AAAAAAAAAA                                http://www -nlpir.nist.gov/projects/duc/pubs/ \\n2003slides/duc2003in tro.pdf  \\nPapineni, K., S. Roukos, T. Ward, and W. -J. Zhu. \\n2001. BLEU : A method for automatic evaluation \\nof machine translation . IBM Research Report \\nRC22176 (W0109 -022). \\nSaggion H., D. Radev, S. Teufel, and W. Lam. \\n2002. Meta -evaluation of summaries in a c ross-\\nlingual env ironment using c ontent -based m etrics. \\nIn Procee dings of COLING -2002 , Taipei, Ta i-\\nwan.  \\nRadev,  D.  S. Teufel, H. Saggion, W. Lam, J. Bli t-\\nzer, A. Gelebi, H. Qi, E. Drabek, and D. Liu. \\n2002. Evalu ation of Text Summarization in a \\nCross -Lingual Info rmation Retrieval Framework . \\nTechnical report, Center for Language and \\nSpeech Processing, Johns Hopkins University, \\nBaltimore, MD, USA.  \\nVan Rijsbergen, C.  J. 1979. Information Retrieval . \\nButterworths. London.  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f036bb11-05c9-4876-94f4-69e5f70d09ea', embedding=None, metadata={'page_label': '1', 'file_name': 'SuperGLUE.pdf', 'file_path': 'data\\\\SuperGLUE.pdf', 'file_type': 'application/pdf', 'file_size': 370981, 'creation_date': '2024-01-30', 'last_modified_date': '2023-12-30', 'last_accessed_date': '2024-01-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='SuperGLUE: A Stickier Benchmark for\\nGeneral-Purpose Language Understanding Systems\\nAlex Wang∗\\nNew York UniversityYada Pruksachatkun∗\\nNew York UniversityNikita Nangia∗\\nNew York University\\nAmanpreet Singh∗\\nFacebook AI ResearchJulian Michael\\nUniversity of WashingtonFelix Hill\\nDeepMindOmer Levy\\nFacebook AI Research\\nSamuel R. Bowman\\nNew York University\\nAbstract\\nIn the last year, new models and methods for pretraining and transfer learning have\\ndriven striking performance improvements across a range of language understand-\\ning tasks. The GLUE benchmark, introduced a little over one year ago, offers\\na single-number metric that summarizes progress on a diverse set of such tasks,\\nbut performance on the benchmark has recently surpassed the level of non-expert\\nhumans, suggesting limited headroom for further research. In this paper we present\\nSuperGLUE, a new benchmark styled after GLUE with a new set of more difﬁ-\\ncult language understanding tasks, a software toolkit, and a public leaderboard.\\nSuperGLUE is available at super.gluebenchmark.com .\\n1 Introduction\\nIn the past year, there has been notable progress across many natural language processing (NLP)\\ntasks, led by methods such as ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018),\\nand BERT (Devlin et al., 2019). The common thread connecting these methods is that they couple\\nself-supervised learning from massive unlabelled text corpora with a recipe for effectively adapting\\nthe resulting model to target tasks. The tasks that have proven amenable to this general approach\\ninclude question answering, sentiment analysis, textual entailment, and parsing, among many others\\n(Devlin et al., 2019; Kitaev and Klein, 2018, i.a.).\\nIn this context, the GLUE benchmark (Wang et al., 2019a) has become a prominent evaluation\\nframework for research towards general-purpose language understanding technologies. GLUE is\\na collection of nine language understanding tasks built on existing public datasets, together with\\nprivate test data, an evaluation server, a single-number target metric, and an accompanying expert-\\nconstructed diagnostic set. GLUE was designed to provide a general-purpose evaluation of language\\nunderstanding that covers a range of training data volumes, task genres, and task formulations. We\\nbelieve it was these aspects that made GLUE particularly appropriate for exhibiting the transfer-\\nlearning potential of approaches like OpenAI GPT and BERT.\\nThe progress of the last twelve months has eroded headroom on the GLUE benchmark dramatically.\\nWhile some tasks (Figure 1) and some linguistic phenomena (Figure 2 in Appendix B) measured\\nin GLUE remain difﬁcult, the current state of the art GLUE Score as of early July 2019 (88.4 from\\nYang et al., 2019) surpasses human performance (87.1 from Nangia and Bowman, 2019) by 1.3\\n∗Equal contribution. Correspondence: glue-benchmark-admin@googlegroups.com\\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='514f360e-a82d-44fa-9250-0495257571f3', embedding=None, metadata={'page_label': '2', 'file_name': 'SuperGLUE.pdf', 'file_path': 'data\\\\SuperGLUE.pdf', 'file_type': 'application/pdf', 'file_size': 370981, 'creation_date': '2024-01-30', 'last_modified_date': '2023-12-30', 'last_accessed_date': '2024-01-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BiLSTM+ELMo+Attn\\nOpenAI GPT\\nBERT + Single-task Adapters\\nBERT (Large)\\nBERT on STILTs\\nBERT + BAM\\nSemBERT\\nSnorkel MeTaL\\nALICE (Large)\\nMT-DNN (ensemble)\\nXLNet-Large (ensemble)0.50.60.70.80.91.01.11.2\\nGLUE Score\\nHuman Performance\\nCoLA\\nSST-2MRPC\\nSTS-B\\nQQP\\nMNLIQNLI\\nRTE\\nWNLIFigure 1: GLUE benchmark performance for submitted systems, rescaled to set human performance\\nto 1.0, shown as a single number score, and broken down into the nine constituent task performances.\\nFor tasks with multiple metrics, we use an average of the metrics. More information on the tasks\\nincluded in GLUE can be found in Wang et al. (2019a) and in Warstadt et al. (2018, CoLA), Socher\\net al. (2013, SST-2), Dolan and Brockett (2005, MRPC), Cer et al. (2017, STS-B), and Williams et al.\\n(2018, MNLI), and Rajpurkar et al. (2016, the original data source for QNLI).\\npoints, and in fact exceeds this human performance estimate on four tasks. Consequently, while there\\nremains substantial scope for improvement towards GLUE’s high-level goals, the original version of\\nthe benchmark is no longer a suitable metric for quantifying such progress.\\nIn response, we introduce SuperGLUE, a new benchmark designed to pose a more rigorous test of\\nlanguage understanding. SuperGLUE has the same high-level motivation as GLUE: to provide a\\nsimple, hard-to-game measure of progress toward general-purpose language understanding technolo-\\ngies for English. We anticipate that signiﬁcant progress on SuperGLUE should require substantive\\ninnovations in a number of core areas of machine learning, including sample-efﬁcient, transfer,\\nmultitask, and unsupervised or self-supervised learning.\\nSuperGLUE follows the basic design of GLUE: It consists of a public leaderboard built around\\neight language understanding tasks, drawing on existing data, accompanied by a single-number\\nperformance metric, and an analysis toolkit. However, it improves upon GLUE in several ways:\\n•More challenging tasks: SuperGLUE retains the two hardest tasks in GLUE. The remain-\\ning tasks were identiﬁed from those submitted to an open call for task proposals and were\\nselected based on difﬁculty for current NLP approaches.\\n•More diverse task formats: The task formats in GLUE are limited to sentence- and\\nsentence-pair classiﬁcation. We expand the set of task formats in SuperGLUE to include\\ncoreference resolution and question answering (QA).\\n•Comprehensive human baselines: We include human performance estimates for all bench-\\nmark tasks, which verify that substantial headroom exists between a strong BERT-based\\nbaseline and human performance.\\n•Improved code support: SuperGLUE is distributed with a new, modular toolkit for work\\non pretraining, multi-task learning, and transfer learning in NLP, built around standard tools\\nincluding PyTorch (Paszke et al., 2017) and AllenNLP (Gardner et al., 2017).\\n•Reﬁned usage rules: The conditions for inclusion on the SuperGLUE leaderboard have\\nbeen revamped to ensure fair competition, an informative leaderboard, and full credit\\nassignment to data and task creators.\\nThe SuperGLUE leaderboard, data, and software tools are available at super.gluebenchmark.com .\\n2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='361e8ea7-d3c1-459c-82d2-baa8fb0275f2', embedding=None, metadata={'page_label': '3', 'file_name': 'SuperGLUE.pdf', 'file_path': 'data\\\\SuperGLUE.pdf', 'file_type': 'application/pdf', 'file_size': 370981, 'creation_date': '2024-01-30', 'last_modified_date': '2023-12-30', 'last_accessed_date': '2024-01-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2 Related Work\\nMuch work prior to GLUE demonstrated that training neural models with large amounts of available\\nsupervision can produce representations that effectively transfer to a broad range of NLP tasks\\n(Collobert and Weston, 2008; Dai and Le, 2015; Kiros et al., 2015; Hill et al., 2016; Conneau and\\nKiela, 2018; McCann et al., 2017; Peters et al., 2018). GLUE was presented as a formal challenge\\naffording straightforward comparison between such task-agnostic transfer learning techniques. Other\\nsimilarly-motivated benchmarks include SentEval (Conneau and Kiela, 2018), which speciﬁcally\\nevaluates ﬁxed-size sentence embeddings, and DecaNLP (McCann et al., 2018), which recasts a set\\nof target tasks into a general question-answering format and prohibits task-speciﬁc parameters. In\\ncontrast, GLUE provides a lightweight classiﬁcation API and no restrictions on model architecture or\\nparameter sharing, which seems to have been well-suited to recent work in this area.\\nSince its release, GLUE has been used as a testbed and showcase by the developers of several\\ninﬂuential models, including GPT (Radford et al., 2018) and BERT (Devlin et al., 2019). As shown\\nin Figure 1, progress on GLUE since its release has been striking. On GLUE, GPT and BERT\\nachieved scores of 72.8 and 80.2 respectively, relative to 66.5 for an ELMo-based model (Peters\\net al., 2018) and 63.7 for the strongest baseline with no multitask learning or pretraining above the\\nword level. Recent models (Liu et al., 2019d; Yang et al., 2019) have clearly surpassed estimates of\\nnon-expert human performance on GLUE (Nangia and Bowman, 2019). The success of these models\\non GLUE has been driven by ever-increasing model capacity, compute power, and data quantity, as\\nwell as innovations in model expressivity (from recurrent to bidirectional recurrent to multi-headed\\ntransformer encoders) and degree of contextualization (from learning representation of words in\\nisolation to using uni-directional contexts and ultimately to leveraging bidirectional contexts).\\nIn parallel to work scaling up pretrained models, several studies have focused on complementary\\nmethods for augmenting performance of pretrained models. Phang et al. (2018) show that BERT can\\nbe improved using two-stage pretraining, i.e., ﬁne-tuning the pretrained model on an intermediate\\ndata-rich supervised task before ﬁne-tuning it again on a data-poor target task. Liu et al. (2019d,c) and\\nBach et al. (2018) get further improvements respectively via multi-task ﬁnetuning and using massive\\namounts of weak supervision. Anonymous (2018) demonstrate that knowledge distillation (Hinton\\net al., 2015; Furlanello et al., 2018) can lead to student networks that outperform their teachers.\\nOverall, the quantity and quality of research contributions aimed at the challenges posed by GLUE\\nunderline the utility of this style of benchmark for machine learning researchers looking to evaluate\\nnew application-agnostic methods on language understanding.\\nLimits to current approaches are also apparent via the GLUE suite. Performance on the GLUE\\ndiagnostic entailment dataset, at 0.42 R3, falls far below the average human performance of 0.80\\nR3reported in the original GLUE publication, with models performing near, or even below, chance\\non some linguistic phenomena (Figure 2, Appendix B). While some initially difﬁcult categories\\nsaw gains from advances on GLUE (e.g., double negation), others remain hard (restrictivity) or\\neven adversarial (disjunction, downward monotonicity). This suggests that even as unsupervised\\npretraining produces ever-better statistical summaries of text, it remains difﬁcult to extract many\\ndetails crucial to semantics without the right kind of supervision. Much recent work has made similar\\nobservations about the limitations of existing pretrained models (Jia and Liang, 2017; Naik et al.,\\n2018; McCoy and Linzen, 2019; McCoy et al., 2019; Liu et al., 2019a,b).\\n3 SuperGLUE Overview\\n3.1 Design Process\\nThe goal of SuperGLUE is to provide a simple, robust evaluation metric of any method capable of\\nbeing applied to a broad range of language understanding tasks. To that end, in designing SuperGLUE,\\nwe identify the following desiderata of tasks in the benchmark:\\n•Task substance: Tasks should test a system’s ability to understand and reason about texts\\nwritten in English.\\n•Task difﬁculty: Tasks should be beyond the scope of current state-of-the-art systems,\\nbut solvable by most college-educated English speakers. We exclude tasks that require\\ndomain-speciﬁc knowledge, e.g. medical notes or scientiﬁc papers.\\n3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1a47d55f-5b70-4580-b412-d0854a261dc5', embedding=None, metadata={'page_label': '4', 'file_name': 'SuperGLUE.pdf', 'file_path': 'data\\\\SuperGLUE.pdf', 'file_type': 'application/pdf', 'file_size': 370981, 'creation_date': '2024-01-30', 'last_modified_date': '2023-12-30', 'last_accessed_date': '2024-01-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 1: The tasks included in SuperGLUE. WSD stands for word sense disambiguation, NLI is\\nnatural language inference, coref. is coreference resolution, and QAis question answering. For\\nMultiRC, we list the number of total answers for 456/83/166 train/dev/test questions. The metrics for\\nMultiRC are binary F1 on all answer-options and exact match.\\nCorpus |Train | | Dev | | Test |Task Metrics Text Sources\\nBoolQ 9427 3270 3245 QA acc. Google queries, Wikipedia\\nCB 250 57 250 NLI acc./F1 various\\nCOPA 400 100 500 QA acc. blogs, photography encyclopedia\\nMultiRC 5100 953 1800 QA F1 a/EM various\\nReCoRD 101k 10k 10k QA F1/EM news (CNN, Daily Mail)\\nRTE 2500 278 300 NLI acc. news, Wikipedia\\nWiC 6000 638 1400 WSD acc. WordNet, VerbNet, Wiktionary\\nWSC 554 104 146 coref. acc. ﬁction books\\n•Evaluability: Tasks must have an automatic performance metric that corresponds well to\\nhuman judgments of output quality. Certain text generation tasks fail to meet this criteria\\ndue to issues surrounding automatic metrics like ROUGE and BLEU (Callison-Burch et al.,\\n2006; Liu et al., 2016, i.a.).\\n•Public data: We require that tasks have existing public training data in order to minimize\\nthe risks involved in newly-created datasets. We also prefer tasks for which we have access\\nto (or could create) a test set with private labels.\\n•Task format: We prefer tasks that had relatively simple input and output formats, to avoid\\nincentivizing the users of the benchmark to create complex task-speciﬁc model architectures.\\nNevertheless, while GLUE is restricted to tasks involving single sentence or sentence pair\\ninputs, for SuperGLUE we expand the scope to consider tasks with longer inputs. This\\nyields a set of tasks that requires understanding individual tokens in context, complete\\nsentences, inter-sentence relations, and entire paragraphs.\\n•License: We require that task data be available under licences that allow use and redistribu-\\ntion for research purposes.\\nTo identify possible tasks for SuperGLUE, we disseminated a public call for task proposals to the\\nNLP community, and received approximately 30 proposals. We ﬁltered these proposals according\\nto our criteria. Many proposals were not suitable due to licensing issues, complex formats, and\\ninsufﬁcient headroom; we provide examples of such tasks in Appendix D. For each of the remaining\\ntasks, we ran a BERT-based baseline and a human baseline, and ﬁltered out tasks which were either\\ntoo challenging for humans without extensive training or too easy for our machine baselines.\\n3.2 Selected Tasks\\nFollowing this process, we arrived at eight tasks to use in SuperGLUE. See Tables 1 and 2 for details\\nand speciﬁc examples of each task.\\nBoolQ (Boolean Questions, Clark et al., 2019) is a QA task where each example consists of a short\\npassage and a yes/no question about the passage. The questions are provided anonymously and\\nunsolicited by users of the Google search engine, and afterwards paired with a paragraph from a\\nWikipedia article containing the answer. Following the original work, we evaluate with accuracy.\\nCB(CommitmentBank, De Marneffe et al., 2019) is a corpus of short texts in which at least one\\nsentence contains an embedded clause. Each of these embedded clauses is annotated with the degree\\nto which it appears the person who wrote the text is committed to the truth of the clause. The resulting\\ntask framed as three-class textual entailment on examples that are drawn from the Wall Street Journal,\\nﬁction from the British National Corpus, and Switchboard. Each example consists of a premise\\ncontaining an embedded clause and the corresponding hypothesis is the extraction of that clause.\\nWe use a subset of the data that had inter-annotator agreement above 80%. The data is imbalanced\\n(relatively fewer neutral examples), so we evaluate using accuracy and F1, where for multi-class F1\\nwe compute the unweighted average of the F1 per class.\\n4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7a72a9a3-fe7b-4aac-86c6-257619a455b6', embedding=None, metadata={'page_label': '5', 'file_name': 'SuperGLUE.pdf', 'file_path': 'data\\\\SuperGLUE.pdf', 'file_type': 'application/pdf', 'file_size': 370981, 'creation_date': '2024-01-30', 'last_modified_date': '2023-12-30', 'last_accessed_date': '2024-01-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 2: Development set examples from the tasks in SuperGLUE. Bold text represents part of the\\nexample format for each task. Text in italics is part of the model input. Underlined text is specially\\nmarked in the input. Text in a monospaced font represents the expected model output.BoolQPassage: Barq’s – Barq’s is an American soft drink. Its brand of root beer is notable for having caffeine.\\nBarq’s, created by Edward Barq and bottled since the turn of the 20th century, is owned by the Barq\\nfamily but bottled by the Coca-Cola Company. It was known as Barq’s Famous Olde Tyme Root Beer\\nuntil 2012.\\nQuestion: is barq’s root beer a pepsi product Answer: NoCBText: B: And yet, uh, I we-, I hope to see employer based, you know, helping out. You know, child, uh,\\ncare centers at the place of employment and things like that, that will help out. A: Uh-huh. B: What do\\nyou think, do you think we are, setting a trend?\\nHypothesis: they are setting a trend Entailment: UnknownCOPAPremise: My body cast a shadow over the grass. Question: What’s the CAUSE for this?\\nAlternative 1: The sun was rising. Alternative 2: The grass was cut.\\nCorrect Alternative: 1MultiRCParagraph: Susan wanted to have a birthday party. She called all of her friends. She has ﬁve friends.\\nHer mom said that Susan can invite them all to the party. Her ﬁrst friend could not go to the party\\nbecause she was sick. Her second friend was going out of town. Her third friend was not so sure if her\\nparents would let her. The fourth friend said maybe. The ﬁfth friend could go to the party for sure. Susan\\nwas a little sad. On the day of the party, all ﬁve friends showed up. Each friend had a present for Susan.\\nSusan was happy and sent each friend a thank you card the next week\\nQuestion: Did Susan’s sick friend recover? Candidate answers: Yes, she recovered (T),No(F),Yes\\n(T),No, she didn’t recover (F),Yes, she was at Susan’s party (T)ReCoRDParagraph: (CNN )Puerto Rico on Sunday overwhelmingly voted for statehood. But Congress, the only\\nbody that can approve new states, will ultimately decide whether the status of the UScommonwealth\\nchanges. Ninety-seven percent of the votes in the nonbinding referendum favored statehood, an increase\\nover the results of a 2012 referendum, ofﬁcial results from the State Electorcal Commission show. It\\nwas the ﬁfth such vote on statehood. \"Today, we the people of Puerto Rico are sending a strong and\\nclear message to the US Congress ... and to the world ... claiming our equal rights as American citizens,\\nPuerto Rico Gov. Ricardo Rossello said in a news release. @highlight Puerto Rico voted Sunday in\\nfavor of US statehood\\nQuery For one, they can truthfully say, “Don’t blame me, I didn’t vote for them, ” when discussing the\\n<placeholder> presidency Correct Entities: USRTEText: Dana Reeve, the widow of the actor Christopher Reeve, has died of lung cancer at age 44,\\naccording to the Christopher Reeve Foundation.\\nHypothesis: Christopher Reeve had an accident. Entailment: FalseWiCContext 1: Room and board .Context 2: He nailed boards across the windows.\\nSense match: FalseWSCText: Mark told Pete many lies about himself, which Pete included in his book. Heshould have been\\nmore truthful. Coreference: False\\nCOPA (Choice of Plausible Alternatives, Roemmele et al., 2011) is a causal reasoning task in which\\na system is given a premise sentence and must determine either the cause or effect of the premise\\nfrom two possible choices. All examples are handcrafted and focus on topics from blogs and a\\nphotography-related encyclopedia. Following the original work, we evaluate using accuracy.\\nMultiRC (Multi-Sentence Reading Comprehension, Khashabi et al., 2018) is a QA task where each\\nexample consists of a context paragraph, a question about that paragraph, and a list of possible\\nanswers. The system must predict which answers are true and which are false. While many QA\\ntasks exist, we use MultiRC because of a number of desirable properties: (i) each question can have\\nmultiple possible correct answers, so each question-answer pair must be evaluated independent of\\nother pairs, (ii) the questions are designed such that answering each question requires drawing facts\\nfrom multiple context sentences, and (iii) the question-answer pair format more closely matches\\nthe API of other tasks in SuperGLUE than the more popular span-extractive QA format does. The\\nparagraphs are drawn from seven domains including news, ﬁction, and historical text. The evaluation\\nmetrics are F1 over all answer-options (F1 a) and exact match of each question’s set of answers (EM).\\n5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f3c0c51d-7584-4c4f-ab74-119b83a08a85', embedding=None, metadata={'page_label': '6', 'file_name': 'SuperGLUE.pdf', 'file_path': 'data\\\\SuperGLUE.pdf', 'file_type': 'application/pdf', 'file_size': 370981, 'creation_date': '2024-01-30', 'last_modified_date': '2023-12-30', 'last_accessed_date': '2024-01-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='ReCoRD (Reading Comprehension with Commonsense Reasoning Dataset, Zhang et al., 2018) is a\\nmultiple-choice QA task. Each example consists of a news article and a Cloze-style question about\\nthe article in which one entity is masked out. The system must predict the masked out entity from a\\ngiven list of possible entities in the provided passage, where the same entity may be expressed using\\nmultiple different surface forms, all of which are considered correct. Articles are drawn from CNN\\nand Daily Mail. Following the original work, we evaluate with max (over all mentions) token-level\\nF1 and exact match (EM).\\nRTE (Recognizing Textual Entailment) datasets come from a series of annual competitions on textual\\nentailment.2RTE is included in GLUE, and we use the same data and format as GLUE: We merge data\\nfrom RTE1 (Dagan et al., 2006), RTE2 (Bar Haim et al., 2006), RTE3 (Giampiccolo et al., 2007), and\\nRTE5 (Bentivogli et al., 2009).3All datasets are combined and converted to two-class classiﬁcation:\\nentailment andnot_entailment . Of all the GLUE tasks, RTE is among those that beneﬁts from\\ntransfer learning the most, with performance jumping from near random-chance ( ∼56%) at the time\\nof GLUE’s launch to 86.3% accuracy (Liu et al., 2019d; Yang et al., 2019) at the time of writing.\\nGiven the nearly eight point gap with respect to human performance, however, the task is not yet\\nsolved by machines, and we expect the remaining gap to be difﬁcult to close.\\nWiC (Word-in-Context, Pilehvar and Camacho-Collados, 2019) is a word sense disambiguation task\\ncast as binary classiﬁcation of sentence pairs. Given two text snippets and a polysemous word that\\nappears in both sentences, the task is to determine whether the word is used with the same sense in\\nboth sentences. Sentences are drawn from WordNet (Miller, 1995), VerbNet (Schuler, 2005), and\\nWiktionary. We follow the original work and evaluate using accuracy.\\nWSC (Winograd Schema Challenge, Levesque et al., 2012) is a coreference resolution task in\\nwhich examples consist of a sentence with a pronoun and a list of noun phrases from the sentence.\\nThe system must determine the correct referrent of the pronoun from among the provided choices.\\nWinograd schemas are designed to require everyday knowledge and commonsense reasoning to solve.\\nGLUE includes a version of WSC recast as NLI, known as WNLI. Until very recently, no substantial\\nprogress had been made on WNLI, with many submissions opting to submit majority class predic-\\ntions.4In the past few months, several works (Kocijan et al., 2019; Liu et al., 2019d) have made rapid\\nprogress via a hueristic data augmentation scheme, raising machine performance to 90.4% accuracy.\\nGiven estimated human performance of ∼96%, there is still a gap between machine and human\\nperformance, which we expect will be relatively difﬁcult to close. We therefore include a version of\\nWSC cast as binary classiﬁcation, where each example consists of a sentence with a marked pronoun\\nand noun, and the task is to determine if the pronoun refers to that noun. The training and validation\\nexamples are drawn from the original WSC data (Levesque et al., 2012), as well as those distributed\\nby the afﬁliated organization Commonsense Reasoning .5The test examples are derived from ﬁction\\nbooks and have been shared with us by the authors of the original dataset. We evaluate using accuracy.\\n3.3 Scoring\\nAs with GLUE, we seek to give a sense of aggregate system performance over all tasks by averaging\\nscores of all tasks. Lacking a fair criterion with which to weight the contributions of each task to\\nthe overall score, we opt for the simple approach of weighing each task equally, and for tasks with\\nmultiple metrics, ﬁrst averaging those metrics to get a task score.\\n3.4 Tools for Model Analysis\\nAnalyzing Linguistic and World Knowledge in Models GLUE includes an expert-constructed,\\ndiagnostic dataset that automatically tests models for a broad range of linguistic, commonsense, and\\nworld knowledge. Each example in this broad-coverage diagnostic is a sentence pair labeled with\\n2Textual entailment is also known as natural language inference, or NLI\\n3RTE4 is not publicly available, while RTE6 and RTE7 do not conform to the standard NLI task.\\n4WNLI is especially difﬁcult due to an adversarial train/dev split: Premise sentences that appear in the\\ntraining set often appear in the development set with a different hypothesis and a ﬂipped label. If a system\\nmemorizes the training set, which was easy due to the small size of the training set, it could perform far below\\nchance on the development set. We remove this adversarial design in our version of WSC by ensuring that no\\nsentences are shared between the training, validation, and test sets.\\n5http://commonsensereasoning.org/disambiguation.html\\n6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0bd479df-aceb-4140-b9d1-fc15082bce47', embedding=None, metadata={'page_label': '7', 'file_name': 'SuperGLUE.pdf', 'file_path': 'data\\\\SuperGLUE.pdf', 'file_type': 'application/pdf', 'file_size': 370981, 'creation_date': '2024-01-30', 'last_modified_date': '2023-12-30', 'last_accessed_date': '2024-01-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='a three-way entailment relation ( entailment ,neutral , orcontradiction ) and tagged with labels that\\nindicate the phenomena that characterize the relationship between the two sentences. Submissions\\nto the GLUE leaderboard are required to include predictions from the submission’s MultiNLI\\nclassiﬁer on the diagnostic dataset, and analyses of the results were shown alongside the main\\nleaderboard. Since this broad-coverage diagnostic task has proved difﬁcult for top models, we retain\\nit in SuperGLUE. However, since MultiNLI is not part of SuperGLUE, we collapse contradiction\\nandneutral into a single not_entailment label, and request that submissions include predictions\\non the resulting set from the model used for the RTE task. We collect non-expert annotations to\\nestimate human performance, following the same procedure we use for the main benchmark tasks\\n(Section 5.2). We estimate an accuracy of 88% and a Matthew’s correlation coefﬁcient (MCC, the\\ntwo-class variant of the R3metric used in GLUE) of 0.77.\\nAnalyzing Gender Bias in Models Recent work has identiﬁed the presence and ampliﬁcation\\nof many social biases in data-driven machine learning models. (Lu et al., 2018; Zhao et al., 2018;\\nKiritchenko and Mohammad, 2018). To promote the detection of such biases, we include Winogender\\n(Rudinger et al., 2018) as an additional diagnostic dataset. Winogender is designed to measure gender\\nbias in coreference resolution systems. We use the Diverse Natural Language Inference Collection\\n(DNC; Poliak et al., 2018) version that casts Winogender as a textual entailment task.6Each example\\nconsists of a premise sentence with a male or female pronoun and a hypothesis giving a possible\\nantecedent of the pronoun. Examples occur in minimal pairs , where the only difference between\\nan example and its pair is the gender of the pronoun in the premise. Performance on Winogender\\nis measured with both accuracy and the gender parity score : the percentage of minimal pairs for\\nwhich the predictions are the same. We note that a system can trivially obtain a perfect gender parity\\nscore by guessing the same class for all examples, so a high gender parity score is meaningless unless\\naccompanied by high accuracy. We collect non-expert annotations to estimate human performance,\\nand observe an accuracy of 99.7% and a gender parity score of 0.99.\\nLike any diagnostic, Winogender has limitations. It offers only positive predictive value: A poor\\nbias score is clear evidence that a model exhibits gender bias, but a good score does not mean that\\nthe model is unbiased. More speciﬁcally, in the DNC version of the task, a low gender parity score\\nmeans that a model’s prediction of textual entailment can be changed with a change in pronouns, all\\nelse equal. It is plausible that there are forms of bias that are relevant to target tasks of interest, but\\nthat do not surface in this setting (Gonen and Goldberg, 2019). In addition, Winogender does not\\ncover all forms of social bias, or even all forms of gender. For instance, the version of the data used\\nhere offers no coverage of gender-neutral they or non-binary pronouns. Despite these limitations, we\\nbelieve that Winogender’s inclusion is worthwhile in providing a coarse sense of how social biases\\nevolve with model performance and for keeping attention on the social ramiﬁcations of NLP models.\\n4 Using SuperGLUE\\nSoftware Tools To facilitate using SuperGLUE, we release jiant (Wang et al., 2019b),7a modular\\nsoftware toolkit, built with PyTorch (Paszke et al., 2017), components from AllenNLP (Gardner\\net al., 2017), and the pytorch-pretrained-bert package.8jiant implements our baselines and\\nsupports the evaluation of custom models and training methods on the benchmark tasks. The toolkit\\nincludes support for existing popular pretrained models such as OpenAI GPT and BERT, as well as\\nsupport for multistage and multitask learning of the kind seen in the strongest models on GLUE.\\nEligibility Any system or method that can produce predictions for the SuperGLUE tasks is eligible\\nfor submission to the leaderboard, subject to the data-use and submission frequency policies stated\\nimmediately below. There are no restrictions on the type of methods that may be used, and there is\\nno requirement that any form of parameter sharing or shared initialization be used across the tasks in\\nthe benchmark. To limit overﬁtting to the private test data, users are limited to a maximum of two\\nsubmissions per day and six submissions per month.\\n6We ﬁlter out 23 examples where the labels are ambiguous\\n7https://github.com/nyu-mll/jiant\\n8https://github.com/huggingface/pytorch-pretrained-BERT\\n7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='95d81ffb-8698-47fe-85f4-79a3db9b660e', embedding=None, metadata={'page_label': '8', 'file_name': 'SuperGLUE.pdf', 'file_path': 'data\\\\SuperGLUE.pdf', 'file_type': 'application/pdf', 'file_size': 370981, 'creation_date': '2024-01-30', 'last_modified_date': '2023-12-30', 'last_accessed_date': '2024-01-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 3: Baseline performance on the SuperGLUE test sets and diagnostics. For CB we report\\naccuracy and macro-average F1. For MultiRC we report F1 on all answer-options and exact match\\nof each question’s set of correct answers. AX bis the broad-coverage diagnostic task, scored using\\nMatthews’ correlation (MCC). AX gis the Winogender diagnostic, scored using accuracy and the\\ngender parity score (GPS). All values are scaled by 100. The Avgcolumn is the overall benchmark\\nscore on non-AX ∗tasks. The bolded numbers reﬂect the best machine performance on task. *MultiRC\\nhas multiple test sets released on a staggered schedule, and these results evaluate on an installation of\\nthe test set that is a subset of ours.\\nModel Avg BoolQ CB COPA MultiRC ReCoRD RTE WiC WSC AX b AXg\\nMetrics Acc. F1/Acc. Acc. F1 a/EM F1/EM Acc. Acc. Acc. MCC GPS Acc.\\nMost Frequent 47.1 62.3 21.7/48.4 50.0 61.1 / 0.3 33.4/32.5 50.3 50.0 65.1 0.0 100.0/ 50.0\\nCBoW 44.3 62.1 49.0/71.2 51.6 0.0 / 0.4 14.0/13.6 49.7 53.0 65.1 -0.4 100.0/ 50.0\\nBERT 69.0 77.4 75.7/83.6 70.6 70.0 / 24.0 72.0/71.3 71.6 69.5 64.3 23.0 97.8 / 51.7\\nBERT++ 71.5 79.0 84.7/90.4 73.8 70.0 / 24.1 72.0/71.3 79.0 69.5 64.3 38.0 99.4 / 51.4\\nOutside Best - 80.4 - / - 84.4 70.4 */24.5*74.8/73.0 82.7 - - - - / -\\nHuman (est.) 89.8 89.0 95.8/98.9 100.0 81.8*/51.9* 91.7/91.3 93.6 80.0 100.0 77.0 99.3 / 99.7\\nData Data for the tasks are available for download through the SuperGLUE site and through a\\ndownload script included with the software toolkit. Each task comes with a standardized training set,\\ndevelopment set, and unlabeled test set. Submitted systems may use any public or private data when\\ndeveloping their systems, with a few exceptions: Systems may only use the SuperGLUE-distributed\\nversions of the task datasets, as these use different train/validation/test splits from other public\\nversions in some cases. Systems also may not use the unlabeled test data for the tasks in system\\ndevelopment in any way, may not use the structured source data that was used to collect the WiC\\nlabels (sense-annotated example sentences from WordNet, VerbNet, and Wiktionary) in any way, and\\nmay not build systems that share information across separate testexamples in any way.\\nWe do not endorse the use of the benchmark data for non-research applications, due to concerns\\nabout socially relevant biases (such as ethnicity–occupation associations) that may be undesirable\\nor legally problematic in deployed systems. Because these biases are evident in texts from a wide\\nvariety of sources and collection methods (e.g., Rudinger et al., 2017), and because none of our task\\ndatasets directly mitigate them, one can reasonably presume that our training sets teach models these\\nbiases to some extent and that our evaluation sets similarly reward models that learn these biases.\\nTo ensure reasonable credit assignment, because we build very directly on prior work, we ask the\\nauthors of submitted systems to directly name and cite the speciﬁc datasets that they use, including the\\nbenchmark datasets . We will enforce this as a requirement for papers to be listed on the leaderboard.\\n5 Experiments\\n5.1 Baselines\\nBERT Our main baselines are built around BERT, variants of which are among the most successful\\napproach on GLUE at the time of writing. Speciﬁcally, we use the bert-large-cased variant.\\nFollowing the practice recommended in Devlin et al. (2019), for each task, we use the simplest\\npossible architecture on top of BERT. We ﬁne-tune a copy of the pretrained BERT model separately\\nfor each task, and leave the development of multi-task learning models to future work. For training,\\nwe use the procedure speciﬁed in Devlin et al. (2019): We use Adam (Kingma and Ba, 2014) with an\\ninitial learning rate of 10−5and ﬁne-tune for a maximum of 10 epochs.\\nFor classiﬁcation tasks with sentence-pair inputs (BoolQ, CB, RTE, WiC), we concatenate the\\nsentences with a [SEP]token, feed the fused input to BERT, and use a logistic regression classiﬁer that\\nsees the representation corresponding to [CLS]. For WiC only, we also concatenate the representation\\nof the marked word to the [CLS]representation. For COPA, MultiRC, and ReCoRD, for each answer\\nchoice, we similarly concatenate the context with that answer choice and feed the resulting sequence\\ninto BERT to produce an answer representation. For COPA, we project these representations into a\\nscalar, and take as the answer the choice with the highest associated scalar. For MultiRC, because\\neach question can have more than one correct answer, we feed each answer representation into\\n8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='23880fbc-5cc3-40ad-a5d9-8fcc548e64b9', embedding=None, metadata={'page_label': '9', 'file_name': 'SuperGLUE.pdf', 'file_path': 'data\\\\SuperGLUE.pdf', 'file_type': 'application/pdf', 'file_size': 370981, 'creation_date': '2024-01-30', 'last_modified_date': '2023-12-30', 'last_accessed_date': '2024-01-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='a logistic regression classiﬁer. For ReCoRD, we also evaluate the probability of each candidate\\nindependent of other candidates, and take the most likely candidate as the model’s prediction. For\\nWSC, which is a span-based task, we use a model inspired by Tenney et al. (2019). Given the BERT\\nrepresentation for each word in the original sentence, we get span representations of the pronoun\\nand noun phrase via a self-attention span-pooling operator (Lee et al., 2017), before feeding it into a\\nlogistic regression classiﬁer.\\nBERT++ We also report results using BERT with additional training on related datasets before\\nﬁne-tuning on the benchmark tasks, following the STILTs two-stage style of transfer learning (Phang\\net al., 2018). Given the productive use of MultiNLI in pretraining and intermediate ﬁne-tuning of\\npretrained language models (Conneau et al., 2017; Phang et al., 2018, i.a.), for CB, RTE, and BoolQ,\\nwe use MultiNLI as a transfer task by ﬁrst using the above procedure on MultiNLI. Similarly, given\\nthe similarity of COPA to SWAG (Zellers et al., 2018), we ﬁrst ﬁne-tune BERT on SWAG. These\\nresults are reported as BERT++. For all other tasks, we reuse the results of BERT ﬁne-tuned on just\\nthat task.\\nSimple Baselines We include a baseline where for each task we simply predict the majority class,9\\nas well as a bag-of-words baseline where each input is represented as an average of its tokens’ GloVe\\nword vectors (the 300D/840B release from Pennington et al., 2014).\\nOutside Best We list the best known result on each task to date, except on tasks which we recast\\n(WSC), resplit (CB), or achieve the best known result (WiC). The outside results for COPA, MultiRC,\\nand RTE are from Sap et al. (2019), Trivedi et al. (2019), and Liu et al. (2019d) respectively.\\n5.2 Human Performance\\nPilehvar and Camacho-Collados (2019), Khashabi et al. (2018), Nangia and Bowman (2019), and\\nZhang et al. (2018) respectively provide estimates for human performance on WiC, MultiRC, RTE,\\nand ReCoRD. For the remaining tasks, including the diagnostic set, we estimate human performance\\nby hiring crowdworker annotators through Amazon’s Mechanical Turk platform to reannotate a\\nsample of each test set. We follow a two step procedure where a crowd worker completes a short\\ntraining phase before proceeding to the annotation phase, modeled after the method used by Nangia\\nand Bowman (2019) for GLUE. For both phases and all tasks, the average pay rate is $23.75/hr.10\\nIn the training phase, workers are provided with instructions on the task, linked to an FAQ page, and\\nare asked to annotate up to 30 examples from the development set. After answering each example,\\nworkers are also asked to check their work against the provided ground truth label. After the training\\nphase is complete, we provide the qualiﬁcation to work on the annotation phase to all workers\\nwho annotated a minimum of ﬁve examples, i.e. completed ﬁve HITs during training and achieved\\nperformance at, or above the median performance across all workers during training.\\nIn the annotation phase, workers are provided with the same instructions as the training phase, and\\nare linked to the same FAQ page. The instructions for all tasks are provided in Appendix C. For the\\nannotation phase we randomly sample 100 examples from the task’s test set, with the exception of\\nWSC where we annotate the full test set. For each example, we collect annotations from ﬁve workers\\nand take a majority vote to estimate human performance. For additional details, see Appendix C.3.\\n5.3 Results\\nTable 3 shows results for all baselines. The simple baselines of predicting the most frequent class\\nand CBOW do not perform well overall, achieving near chance performance for several of the tasks.\\nUsing BERT increases the average SuperGLUE score by 25 points, attaining signiﬁcant gains on\\nall of the benchmark tasks, particularly MultiRC, ReCoRD, and RTE. On WSC, BERT actually\\nperforms worse than the simple baselines, likely due to the small size of the dataset and the lack of\\ndata augmentation. Using MultiNLI as an additional source of supervision for BoolQ, CB, and RTE\\nleads to a 2-5 point improvement on all tasks. Using SWAG as a transfer task for COPA sees an 8\\npoint improvement.\\n9For ReCoRD, we predict the entity that has the highest F1 with the other entity options.\\n10This estimate is taken from https://turkerview.com .\\n9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='34f3a7ac-64da-488a-9427-345c1a895818', embedding=None, metadata={'page_label': '10', 'file_name': 'SuperGLUE.pdf', 'file_path': 'data\\\\SuperGLUE.pdf', 'file_type': 'application/pdf', 'file_size': 370981, 'creation_date': '2024-01-30', 'last_modified_date': '2023-12-30', 'last_accessed_date': '2024-01-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Our best baselines still lag substantially behind human performance. On average, there is a nearly 20\\npoint gap between BERT++ and human performance. The largest gap is on WSC, with a 35 point\\ndifference between the best model and human performance. The smallest margins are on BoolQ,\\nCB, RTE, and WiC, with gaps of around 10 points on each of these. We believe these gaps will be\\nchallenging to close: On WSC and COPA, human performance is perfect. On three other tasks, it is\\nin the mid-to-high 90s. On the diagnostics, all models continue to lag signiﬁcantly behind humans.\\nThough all models obtain near perfect gender parity scores on Winogender, this is due to the fact that\\nthey are obtaining accuracy near that of random guessing.\\n6 Conclusion\\nWe present SuperGLUE, a new benchmark for evaluating general-purpose language understanding\\nsystems. SuperGLUE updates the GLUE benchmark by identifying a new set of challenging NLU\\ntasks, as measured by the difference between human and machine baselines. The set of eight tasks in\\nour benchmark emphasizes diverse task formats and low-data training data tasks, with nearly half the\\ntasks having fewer than 1k examples and all but one of the tasks having fewer than 10k examples.\\nWe evaluate BERT-based baselines and ﬁnd that they still lag behind humans by nearly 20 points.\\nGiven the difﬁculty of SuperGLUE for BERT, we expect that further progress in multi-task, transfer,\\nand unsupervised/self-supervised learning techniques will be necessary to approach human-level per-\\nformance on the benchmark. Overall, we argue that SuperGLUE offers a rich and challenging testbed\\nfor work developing new general-purpose machine learning methods for language understanding.\\n7 Acknowledgments\\nWe thank the original authors of the included datasets in SuperGLUE for their cooperation in the\\ncreation of the benchmark, as well as those who proposed tasks and datasets that we ultimately could\\nnot include.\\nThis work was made possible in part by a donation to NYU from Eric and Wendy Schmidt made by\\nrecommendation of the Schmidt Futures program. We gratefully acknowledge the support of the\\nNVIDIA Corporation with the donation of a Titan V GPU used at NYU for this research. AW is\\nsupported by the National Science Foundation Graduate Research Fellowship Program under Grant\\nNo. DGE 1342536. Any opinions, ﬁndings, and conclusions or recommendations expressed in this\\nmaterial are those of the author(s) and do not necessarily reﬂect the views of the National Science\\nFoundation.\\nReferences\\nAnonymous. Bam! Born-again multi-task networks for natural language understanding. Anonymous\\npreprint under review, 2018. URL https://openreview.net/forum?id=SylnYlqKw4 .\\nStephen H. Bach, Daniel Rodriguez, Yintao Liu, Chong Luo, Haidong Shao, Cassandra Xia, Souvik\\nSen, Alexander Ratner, Braden Hancock, Houman Alborzi, Rahul Kuchhal, Christopher Ré, and\\nRob Malkin. Snorkel drybell: A case study in deploying weak supervision at industrial scale. In\\nSIGMOD . ACM, 2018.\\nRoy Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and\\nIdan Szpektor. The second PASCAL recognising textual entailment challenge. In Proceedings\\nof the Second PASCAL Challenges Workshop on Recognising Textual Entailment , 2006. URL\\nhttp://u.cs.biu.ac.il/~nlp/RTE2/Proceedings/01.pdf .\\nLuisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. The\\nﬁfth PASCAL recognizing textual entailment challenge. In Textual Analysis Conference (TAC) ,\\n2009. URL http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.232.1231 .\\nSven Buechel, Anneke Buffone, Barry Slaff, Lyle Ungar, and João Sedoc. Modeling empathy and\\ndistress in reaction to news stories. In Proceedings of the 2018 Conference on Empirical Methods\\nin Natural Language Processing (EMNLP) , 2018.\\n10', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6ad2c219-139e-46cd-84ae-d43686122818', embedding=None, metadata={'page_label': '11', 'file_name': 'SuperGLUE.pdf', 'file_path': 'data\\\\SuperGLUE.pdf', 'file_type': 'application/pdf', 'file_size': 370981, 'creation_date': '2024-01-30', 'last_modified_date': '2023-12-30', 'last_accessed_date': '2024-01-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chris Callison-Burch, Miles Osborne, and Philipp Koehn. Re-evaluation the role of bleu in machine\\ntranslation research. In Proceedings of the Conference of the European Chapter of the Association\\nfor Computational Linguistics (EACL) . Association for Computational Linguistics, 2006. URL\\nhttps://www.aclweb.org/anthology/E06-1032 .\\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task\\n1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings\\nof the 11th International Workshop on Semantic Evaluation (SemEval-2017) . Association for\\nComputational Linguistics, 2017. doi: 10.18653/v1/S17-2001. URL https://www.aclweb.\\norg/anthology/S17-2001 .\\nEunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and Luke\\nZettlemoyer. QuAC: Question answering in context. In Proceedings of the 2018 Conference on\\nEmpirical Methods in Natural Language Processing (EMNLP) . Association for Computational\\nLinguistics, 2018a.\\nEunsol Choi, Omer Levy, Yejin Choi, and Luke Zettlemoyer. Ultra-ﬁne entity typing. In Proceedings\\nof the Association for Computational Linguistics (ACL) . Association for Computational Linguistics,\\n2018b. URL https://www.aclweb.org/anthology/P18-1009 .\\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina\\nToutanova. Boolq: Exploring the surprising difﬁculty of natural yes/no questions. In Proceedings\\nof the 2019 Conference of the North American Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 2924–2936,\\n2019.\\nRonan Collobert and Jason Weston. A uniﬁed architecture for natural language processing: Deep\\nneural networks with multitask learning. In Proceedings of the 25th International Conference on\\nMachine Learning (ICML) . Association for Computing Machinery, 2008. URL https://dl.acm.\\norg/citation.cfm?id=1390177 .\\nAlexis Conneau and Douwe Kiela. SentEval: An evaluation toolkit for universal sentence representa-\\ntions. In Proceedings of the 11th Language Resources and Evaluation Conference . European Lan-\\nguage Resource Association, 2018. URL https://www.aclweb.org/anthology/L18-1269 .\\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Loïc Barrault, and Antoine Bordes. Super-\\nvised learning of universal sentence representations from natural language inference data. In\\nProceedings of the 2017 Conference on Empirical Methods in Natural Language Processing\\n(EMNLP) . Association for Computational Linguistics, 2017. doi: 10.18653/v1/D17-1070. URL\\nhttps://www.aclweb.org/anthology/D17-1070 .\\nIdo Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL recognising textual entail-\\nment challenge. In Machine Learning Challenges. Evaluating Predictive Uncertainty, Vi-\\nsual Object Classiﬁcation, and Recognising Textual Entailment . Springer, 2006. URL https:\\n//link.springer.com/chapter/10.1007/11736790_9 .\\nAndrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Advances in Neural\\nInformation Processing Systems (NeurIPS) . Curran Associates, Inc., 2015. URL http://papers.\\nnips.cc/paper/5949-semi-supervised-sequence-learning.pdf .\\nMarie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. The CommitmentBank:\\nInvestigating projection in naturally occurring discourse. 2019. To appear in Proceedings of Sinn\\nund Bedeutung 23 . Data can be found at https://github.com/mcdm/CommitmentBank/ .\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\\nbidirectional transformers for language understanding. In Proceedings of the Conference of the\\nNorth American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies (NAACL-HLT) . Association for Computational Linguistics, 2019. URL https:\\n//arxiv.org/abs/1810.04805 .\\nWilliam B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases.\\nInProceedings of IWP , 2005.\\n11', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='897ee45c-c822-4461-9271-155b2952bcd7', embedding=None, metadata={'page_label': '12', 'file_name': 'SuperGLUE.pdf', 'file_path': 'data\\\\SuperGLUE.pdf', 'file_type': 'application/pdf', 'file_size': 370981, 'creation_date': '2024-01-30', 'last_modified_date': '2023-12-30', 'last_accessed_date': '2024-01-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Manaal Faruqui and Dipanjan Das. Identifying well-formed natural language questions. In Pro-\\nceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP) .\\nAssociation for Computational Linguistics, 2018. URL https://www.aclweb.org/anthology/\\nD18-1091 .\\nTommaso Furlanello, Zachary C Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar.\\nBorn again neural networks. International Conference on Machine Learning (ICML) , 2018. URL\\nhttp://proceedings.mlr.press/v80/furlanello18a.html .\\nMatt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew\\nPeters, Michael Schmitz, and Luke S. Zettlemoyer. AllenNLP: A deep semantic natural language\\nprocessing platform. In Proceedings of Workshop for NLP Open Source Software , 2017. URL\\nhttps://www.aclweb.org/anthology/W18-2501 .\\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third PASCAL recognizing\\ntextual entailment challenge. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment\\nand Paraphrasing . Association for Computational Linguistics, 2007.\\nHila Gonen and Yoav Goldberg. Lipstick on a pig: Debiasing methods cover up systematic\\ngender biases in word embeddings but do not remove them. In Proceedings of the 2019\\nConference of the North American Chapter of the Association for Computational Linguistics:\\nHuman Language Technologies, Volume 1 (Long and Short Papers) , pages 609–614, Min-\\nneapolis, Minnesota, June 2019. Association for Computational Linguistics. URL https:\\n//www.aclweb.org/anthology/N19-1061 .\\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. Learning distributed representations of sentences\\nfrom unlabelled data. In Proceedings of the Conference of the North American Chapter of\\nthe Association for Computational Linguistics: Human Language Technologies (NAACL-HLT) .\\nAssociation for Computational Linguistics, 2016. doi: 10.18653/v1/N16-1162. URL https:\\n//www.aclweb.org/anthology/N16-1162 .\\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv\\npreprint 1503.02531 , 2015. URL https://arxiv.org/abs/1503.02531 .\\nRobin Jia and Percy Liang. Adversarial examples for evaluating reading comprehension systems. In\\nProceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP) .\\nAssociation for Computational Linguistics, 2017. doi: 10.18653/v1/D17-1215. URL https:\\n//www.aclweb.org/anthology/D17-1215 .\\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Looking\\nbeyond the surface: A challenge set for reading comprehension over multiple sentences. In\\nProceedings of the Conference of the North American Chapter of the Association for Computa-\\ntional Linguistics: Human Language Technologies (NAACL-HLT) . Association for Computational\\nLinguistics, 2018. URL https://www.aclweb.org/anthology/papers/N/N18/N18-1023/ .\\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\\n1412.6980 , 2014. URL https://arxiv.org/abs/1412.6980 .\\nSvetlana Kiritchenko and Saif Mohammad. Examining gender and race bias in two hundred sentiment\\nanalysis systems. In Proceedings of the Seventh Joint Conference on Lexical and Computational\\nSemantics . Association for Computational Linguistics, 2018. doi: 10.18653/v1/S18-2005. URL\\nhttps://www.aclweb.org/anthology/S18-2005 .\\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. Skip-thought vectors. In Advances in neural information processing systems ,\\n2015.\\nNikita Kitaev and Dan Klein. Multilingual constituency parsing with self-attention and pre-training.\\narXiv preprint 1812.11760 , 2018. URL https://arxiv.org/abs/1812.11760 .\\nVid Kocijan, Ana-Maria Cretu, Oana-Maria Camburu, Yordan Yordanov, and Thomas Lukasiewicz.\\nA surprisingly robust trick for winograd schema challenge. arXiv preprint 1905.06290 , 2019.\\n12', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3cd0f1d7-9c0f-40c1-9138-bf2b6e87ad9b', embedding=None, metadata={'page_label': '13', 'file_name': 'SuperGLUE.pdf', 'file_path': 'data\\\\SuperGLUE.pdf', 'file_type': 'application/pdf', 'file_size': 370981, 'creation_date': '2024-01-30', 'last_modified_date': '2023-12-30', 'last_accessed_date': '2024-01-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Kenton Lee, Luheng He, Mike Lewis, and Luke Zettlemoyer. End-to-end neural coreference\\nresolution. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language\\nProcessing . Association for Computational Linguistics, September 2017. doi: 10.18653/v1/\\nD17-1018. URL https://www.aclweb.org/anthology/D17-1018 .\\nHector Levesque, Ernest Davis, and Leora Morgenstern. The Winograd schema challenge. In\\nThirteenth International Conference on the Principles of Knowledge Representation and Reasoning ,\\n2012. URL http://dl.acm.org/citation.cfm?id=3031843.3031909 .\\nChia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Noseworthy, Laurent Charlin, and Joelle Pineau.\\nHow not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics\\nfor dialogue response generation. In Proceedings of the 2016 Conference on Empirical Methods in\\nNatural Language Processing . Association for Computational Linguistics, 2016. doi: 10.18653/\\nv1/D16-1230. URL https://www.aclweb.org/anthology/D16-1230 .\\nNelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, and Noah A. Smith. Linguistic\\nknowledge and transferability of contextual representations. In Proceedings of the Conference of\\nthe North American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies (NAACL-HLT) . Association for Computational Linguistics, 2019a. URL https:\\n//arxiv.org/abs/1903.08855 .\\nNelson F. Liu, Roy Schwartz, and Noah A. Smith. Inoculation by ﬁne-tuning: A method for\\nanalyzing challenge datasets. In Proceedings of the Conference of the North American Chapter\\nof the Association for Computational Linguistics: Human Language Technologies (NAACL-\\nHLT) . Association for Computational Linguistics, 2019b. URL https://arxiv.org/abs/1904.\\n02668 .\\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Improving multi-task deep neural\\nnetworks via knowledge distillation for natural language understanding. arXiv preprint 1904.09482 ,\\n2019c. URL http://arxiv.org/abs/1904.09482 .\\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks for\\nnatural language understanding. arXiv preprint 1901.11504 , 2019d.\\nKaiji Lu, Piotr Mardziel, Fangjing Wu, Preetam Amancharla, and Anupam Datta. Gender bias in\\nneural natural language processing. arXiv preprint 1807.11714 , 2018. URL http://arxiv.org/\\nabs/1807.11714 .\\nBryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in transla-\\ntion: Contextualized word vectors. In Advances in Neural Information Processing Sys-\\ntems (NeurIPS) . Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/\\n7209-learned-in-translation-contextualized-word-vectors.pdf .\\nBryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language\\ndecathlon: Multitask learning as question answering. arXiv preprint 1806.08730 , 2018. URL\\nhttps://arxiv.org/abs/1806.08730 .\\nR. Thomas McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnosing syntactic\\nheuristics in natural language inference. In Proceedings of the Association for Computational\\nLinguistics (ACL) . Association for Computational Linguistics, 2019. URL https://arxiv.org/\\nabs/1902.01007 .\\nRichard T. McCoy and Tal Linzen. Non-entailed subsequences as a challenge for natural language\\ninference. In Proceedings of the Society for Computational in Linguistics (SCiL) 2019 , 2019. URL\\nhttps://scholarworks.umass.edu/scil/vol2/iss1/46/ .\\nGeorge A Miller. WordNet: a lexical database for english. Communications of the ACM , 1995. URL\\nhttps://www.aclweb.org/anthology/H94-1111 .\\nAakanksha Naik, Abhilasha Ravichander, Norman M. Sadeh, Carolyn Penstein Rosé, and Graham\\nNeubig. Stress test evaluation for natural language inference. In International Conference on\\nComputational Linguistics (COLING) , 2018.\\n13', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7e29544f-3452-484e-a213-66f6df6cb293', embedding=None, metadata={'page_label': '14', 'file_name': 'SuperGLUE.pdf', 'file_path': 'data\\\\SuperGLUE.pdf', 'file_type': 'application/pdf', 'file_size': 370981, 'creation_date': '2024-01-30', 'last_modified_date': '2023-12-30', 'last_accessed_date': '2024-01-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Nikita Nangia and Samuel R. Bowman. Human vs. Muppet: A conservative estimate of hu-\\nman performance on the GLUE benchmark. In Proceedings of the Association of Compu-\\ntational Linguistics (ACL) . Association for Computational Linguistics, 2019. URL https:\\n//woollysocks.github.io/assets/GLUE_Human_Baseline.pdf .\\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,\\nZeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in\\nPyTorch. In Advances in Neural Information Processing Systems (NeurIPS) . Curran Associates,\\nInc., 2017. URL https://openreview.net/pdf?id=BJJsrmfCZ .\\nJeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word\\nrepresentation. In Proceedings of the Conference on Empirical Methods in Natural Language Pro-\\ncessing (EMNLP) . Association for Computational Linguistics, 2014. doi: 10.3115/v1/D14-1162.\\nURL https://www.aclweb.org/anthology/D14-1162 .\\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\\nLuke Zettlemoyer. Deep contextualized word representations. In Proceedings of the Conference of\\nthe North American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies (NAACL-HLT) . Association for Computational Linguistics, 2018. doi: 10.18653/v1/\\nN18-1202. URL https://www.aclweb.org/anthology/N18-1202 .\\nJason Phang, Thibault Févry, and Samuel R Bowman. Sentence encoders on STILTs: Supplementary\\ntraining on intermediate labeled-data tasks. arXiv preprint 1811.01088 , 2018. URL https:\\n//arxiv.org/abs/1811.01088 .\\nMohammad Taher Pilehvar and Jose Camacho-Collados. WiC: The word-in-context dataset for\\nevaluating context-sensitive meaning representations. In Proceedings of the Conference of the\\nNorth American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies (NAACL-HLT) . Association for Computational Linguistics, 2019. URL https:\\n//arxiv.org/abs/1808.09121 .\\nAdam Poliak, Aparajita Haldar, Rachel Rudinger, J. Edward Hu, Ellie Pavlick, Aaron Steven White,\\nand Benjamin Van Durme. Collecting diverse natural language inference problems for sentence\\nrepresentation evaluation. In Proceedings of the 2018 Conference on Empirical Methods in\\nNatural Language Processing . Association for Computational Linguistics, 2018. URL https:\\n//www.aclweb.org/anthology/D18-1007 .\\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language un-\\nderstanding by generative pre-training, 2018. Unpublished ms. available through a link at\\nhttps://blog.openai.com/language-unsupervised/ .\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions\\nfor machine comprehension of text. In Proceedings of the Conference on Empirical Methods in\\nNatural Language Processing (EMNLP) . Association for Computational Linguistics, 2016. doi:\\n10.18653/v1/D16-1264. URL http://aclweb.org/anthology/D16-1264 .\\nMelissa Roemmele, Cosmin Adrian Bejan, and Andrew S. Gordon. Choice of plausible alternatives:\\nAn evaluation of commonsense causal reasoning. In 2011 AAAI Spring Symposium Series , 2011.\\nRachel Rudinger, Chandler May, and Benjamin Van Durme. Social bias in elicited natural language\\ninferences. In Proceedings of the First ACL Workshop on Ethics in Natural Language Processing .\\nAssociation for Computational Linguistics, 2017. doi: 10.18653/v1/W17-1609. URL https:\\n//www.aclweb.org/anthology/W17-1609 .\\nRachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme. Gender bias in\\ncoreference resolution. In Proceedings of the 2018 Conference of the North American Chapter\\nof the Association for Computational Linguistics: Human Language Technologies . Association\\nfor Computational Linguistics, 2018. doi: 10.18653/v1/N18-2002. URL https://www.aclweb.\\norg/anthology/N18-2002 .\\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense\\nreasoning about social interactions. arXiv preprint 1904.09728 , 2019. URL https://arxiv.\\norg/abs/1904.09728 .\\n14', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ba3ff79e-0163-4c16-871e-b9fc1326d9ef', embedding=None, metadata={'page_label': '15', 'file_name': 'SuperGLUE.pdf', 'file_path': 'data\\\\SuperGLUE.pdf', 'file_type': 'application/pdf', 'file_size': 370981, 'creation_date': '2024-01-30', 'last_modified_date': '2023-12-30', 'last_accessed_date': '2024-01-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Nathan Schneider and Noah A Smith. A corpus and model integrating multiword expressions and\\nsupersenses. In Proceedings of the Conference of the North American Chapter of the Association\\nfor Computational Linguistics: Human Language Technologies (NAACL-HLT) . Association for\\nComputational Linguistics, 2015. URL https://www.aclweb.org/anthology/N15-1177 .\\nKarin Kipper Schuler. Verbnet: A Broad-coverage, Comprehensive Verb Lexicon . PhD thesis, 2005.\\nURL http://verbs.colorado.edu/~kipper/Papers/dissertation.pdf .\\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng,\\nand Christopher. Potts. Recursive deep models for semantic compositionality over a sentiment\\ntreebank. In Proceedings of the Conference on Empirical Methods in Natural Language Processing\\n(EMNLP) . Association for Computational Linguistics, 2013. URL https://www.aclweb.org/\\nanthology/D13-1170 .\\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim,\\nBenjamin Van Durme, Sam Bowman, Dipanjan Das, and Ellie Pavlick. What do you learn from\\ncontext? probing for sentence structure in contextualized word representations. 2019. URL\\nhttps://openreview.net/forum?id=SJzSgnRcKX .\\nHarsh Trivedi, Heeyoung Kwon, Tushar Khot, Ashish Sabharwal, and Niranjan Balasubramanian.\\nRepurposing entailment for multi-hop question answering tasks, 2019. URL https://arxiv.\\norg/abs/1904.09380 .\\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\\nGLUE: A multi-task benchmark and analysis platform for natural language understanding. In\\nInternational Conference on Learning Representations , 2019a. URL https://openreview.\\nnet/forum?id=rJ4km2R5t7 .\\nAlex Wang, Ian F. Tenney, Yada Pruksachatkun, Katherin Yu, Jan Hula, Patrick Xia, Raghu Pappagari,\\nShuning Jin, R. Thomas McCoy, Roma Patel, Yinghui Huang, Jason Phang, Edouard Grave,\\nNajoung Kim, Phu Mon Htut, Thibault F’evry, Berlin Chen, Nikita Nangia, Haokun Liu, , Anhad\\nMohananey, Shikha Bordia, Ellie Pavlick, and Samuel R. Bowman. jiant 1.0: A software toolkit\\nfor research on general-purpose text understanding models. http://jiant.info/ , 2019b.\\nAlex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments.\\narXiv preprint 1805.12471 , 2018. URL https://arxiv.org/abs/1805.12471 .\\nKellie Webster, Marta Recasens, Vera Axelrod, and Jason Baldridge. Mind the GAP: A balanced\\ncorpus of gendered ambiguous pronouns. Transactions of the Association for Computational\\nLinguistics (TACL) , 2018. URL https://www.aclweb.org/anthology/Q18-1042 .\\nAdina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for\\nsentence understanding through inference. In Proceedings of the Conference of the North American\\nChapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-\\nHLT) . Association for Computational Linguistics, 2018. URL http://aclweb.org/anthology/\\nN18-1101 .\\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V .\\nLe. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint\\n1906.0823 , 2019.\\nFabio Massimo Zanzotto and Lorenzo Ferrone. Have you lost the thread? discovering ongoing\\nconversations in scattered dialog blocks. ACM Transactions on Interactive Intelligent Systems\\n(TiiS) , 2017.\\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. SWAG: A large-scale adversarial dataset\\nfor grounded commonsense inference. 2018. URL https://www.aclweb.org/anthology/\\nD18-1009 .\\nSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme.\\nRecord: Bridging the gap between human and machine commonsense reading comprehension.\\narXiv preprint 1810.12885 , 2018.\\n15', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9d7fe355-62f9-4870-b410-ac24ac987136', embedding=None, metadata={'page_label': '16', 'file_name': 'SuperGLUE.pdf', 'file_path': 'data\\\\SuperGLUE.pdf', 'file_type': 'application/pdf', 'file_size': 370981, 'creation_date': '2024-01-30', 'last_modified_date': '2023-12-30', 'last_accessed_date': '2024-01-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Yuan Zhang, Jason Baldridge, and Luheng He. PAWS: Paraphrase adversaries from word scrambling.\\narXiv preprint 1904.01130 , 2019. URL https://arxiv.org/abs/1904.01130 .\\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. Gender bias in\\ncoreference resolution: Evaluation and debiasing methods. In Proceedings of the 2018 Conference\\nof the North American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies . Association for Computational Linguistics, 2018. doi: 10.18653/v1/N18-2003. URL\\nhttps://www.aclweb.org/anthology/N18-2003 .\\n16', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d08712d6-f136-46aa-8282-0fb6e9d4c1d0', embedding=None, metadata={'page_label': '17', 'file_name': 'SuperGLUE.pdf', 'file_path': 'data\\\\SuperGLUE.pdf', 'file_type': 'application/pdf', 'file_size': 370981, 'creation_date': '2024-01-30', 'last_modified_date': '2023-12-30', 'last_accessed_date': '2024-01-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 4: Baseline performance on the SuperGLUE development.\\nModel Avg BoolQ CB COPA MultiRC ReCoRD RTE WiC WSC\\nMetrics Acc. Acc./F1 Acc. F1 a/EM F1/EM Acc. Acc. Acc.\\nMost Frequent Class 47.7 62.2 50 /22.2 55 59.9/ 0.8 32.4/31.5 52.7 50.0 63.5\\nCBOW 47.7 62.4 71.4/49.6 63.0 20.3/ 0.3 14.4/13.8 54.2 55.3 61.5\\nBERT 72.2 77.7 94.6/93.7 69.0 70.5/24.7 70.6/69.8 75.8 74.9 68.3\\nBERT++ 74.6 80.1 96.4/95.0 78.0 70.5/24.7 70.6/69.8 82.3 74.9 68.3\\nA Development Set Results\\nIn Table 4, we present results of the baselines on the SuperGLUE tasks development sets.\\nB Performance on GLUE Diagnostics\\nFigure 2 shows the performance on the GLUE diagnostics dataset for systems submitted to the public\\nleaderboard.\\nDisjunction Downward Monotone Restrictivity Double Negation Prepositional Phrases60\\n40\\n20\\n020406080\\nChance\\nBiLSTM+ELMo+Attn\\nOpenAI GPT\\nBERT + Single-task Adapters\\nBERT (Large)\\nBERT on STILTsBERT + BAM\\nSemBERT\\nSnorkel MeTaL\\nALICE (Large)\\nMT-DNN (ensemble)\\nXLNet-Large (ensemble)\\nFigure 2: Performance of GLUE submissions on selected diagnostic categories, reported using the\\nR3metric scaled up by 100, as in Wang et al. (2019a, see paper for a description of the categories).\\nSome initially difﬁcult categories, like double negation, saw gains from advances on GLUE, but\\nothers remain hard (restrictivity) or even adversarial (disjunction, downward monotone).\\nC Instructions to Crowd Workers\\nC.1 Training Phase Instructions\\nFor collecting data to establish human performance on the SuperGLUE tasks, we follow a two\\nstep procedure where we ﬁrst provide some training to the crowd workers before they proceed to\\nannotation. In the training step, we provide workers with brief instructions about the training phase.\\nAn example of these instructions is given Table 5. These training instructions are the same across\\ntasks, only the task name in the instructions is changed.\\n17', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e825fa1f-4b21-4de8-b9b3-7debf984ab4a', embedding=None, metadata={'page_label': '18', 'file_name': 'SuperGLUE.pdf', 'file_path': 'data\\\\SuperGLUE.pdf', 'file_type': 'application/pdf', 'file_size': 370981, 'creation_date': '2024-01-30', 'last_modified_date': '2023-12-30', 'last_accessed_date': '2024-01-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='C.2 Task Instructions\\nDuring training and annotation for each task, we provide workers with brief instructions tailored to\\nthe task. We also link workers to an FAQ page for the task. Tables 6, 7, 8, and 9, show the instructions\\nwe used for all four tasks: COPA, CommitmentBank, WSC, and BoolQ respectively. The instructions\\ngiven to crowd workers for annotations on the diagnostic and bias diagnostic datasets are shown in\\nTable 11.\\nWe collected data to produce conservative estimates for human performance on several tasks that\\nwe did not ultimately include in our benchmark, including GAP (Webster et al., 2018), PAWS\\n(Zhang et al., 2019), Quora Insincere Questions,11Ultraﬁne Entity Typing (Choi et al., 2018b), and\\nEmpathetic Reactions datasets (Buechel et al., 2018). The instructions we used for these tasks are\\nshown in Tables 12, 13, 14, 15, and 16.\\nC.3 Task Speciﬁc Details\\nFor WSC and COPA we provide annotators with a two way classiﬁcation problem. We then use\\nmajority vote across annotations to calculate human performance.\\nCommitmentBank We follow the authors in providing annotators with a 7-way classiﬁcation\\nproblem. We then collapse the annotations into 3 classes by using the same ranges for bucketing used\\nby De Marneffe et al. (2019). We then use majority vote to get human performance numbers on the\\ntask.\\nFurthermore, for training on CommitmentBank we randomly sample examples from the low inter-\\nannotator agreement portion of the CommitmentBank data that is not included in the benchmark\\nversion of the task. These low agreement examples are generally harder to classify since they are\\nmore ambiguous.\\nDiagnostic Dataset Since the diagnostic dataset does not come with accompanying training data,\\nwe train our workers on examples from RTE’s development set. RTE is also a textual entailment\\ntask and is the most closely related task in the main benchmark. Providing the crowd workers with\\ntraining on RTE enables them to learn label deﬁnitions which should generalize to the diagnostic\\ndataset.\\nUltraﬁne Entity Typing We cast the task into a binary classiﬁcation problem to make it an easier\\ntask for non-expert crowd workers. We work in cooperation with the authors of the dataset (Choi\\net al., 2018b) to do this reformulation: We give workers one possible tag for a word or phrase and\\nasked them to classify the tag as being applicable or not.\\nThe authors used WordNet (Miller, 1995) to expand the set of labels to include synonyms and\\nhypernyms from WordNet. They then asked ﬁve annotators to validate these tags. The tags from this\\nvalidation had high agreement, and were included in the publicly available Ultraﬁne Entity Typing\\ndataset,12This constitutes our set of positive examples. The rest of the tags from the validation\\nprocedure that are not in the public dataset constitute our negative examples.\\nGAP For the Gendered Ambiguous Pronoun Coreference task (GAP, Webster et al., 2018), we\\nsimpliﬁed the task by providing noun phrase spans as part of the input, thus reducing the original\\nstructure prediction task to a classiﬁcation task. This task was presented to crowd workers as a three\\nway classiﬁcation problem: Choose span A, B, or neither.\\nD Excluded Tasks\\nIn this section we provide some examples of tasks that we evaluated for inclusion but ultimately could\\nnot include. We report on these excluded tasks only with the permission of their authors. We turned\\ndown many medical text datasets because they are usually only accessible with explicit permission\\nand credentials from the data owners.\\n11https://www.kaggle.com/c/quora-insincere-questions-classification/data\\n12https://homes.cs.washington.edu/~eunsol/open_entity.html\\n18', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a0d3376f-1a2b-496f-8d82-c8b0dfe98f27', embedding=None, metadata={'page_label': '19', 'file_name': 'SuperGLUE.pdf', 'file_path': 'data\\\\SuperGLUE.pdf', 'file_type': 'application/pdf', 'file_size': 370981, 'creation_date': '2024-01-30', 'last_modified_date': '2023-12-30', 'last_accessed_date': '2024-01-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Tasks like QuAC (Choi et al., 2018a) and STREUSLE (Schneider and Smith, 2015) differed substan-\\ntially from the format of other tasks in our benchmark, which we worried would incentivize users\\nto spend signiﬁcant effort on task-speciﬁc model designs, rather than focusing on general-purpose\\ntechniques. It was challenging to train annotators to do well on Quora Insincere Questions13, Empa-\\nthetic Reactions (Buechel et al., 2018), and a recast version of Ultra-Fine Entity Typing (Choi et al.,\\n2018b, see Appendix C.3 for details), leading to low human performance. BERT achieved very high\\nor superhuman performance on Query Well-Formedness (Faruqui and Das, 2018), PAWS (Zhang\\net al., 2019), Discovering Ongoing Conversations (Zanzotto and Ferrone, 2017), and GAP (Webster\\net al., 2018).\\nDuring the process of selecting tasks for our benchmark, we collected human performance baselines\\nand run BERT-based machine baselines for some tasks that we ultimately excluded from our task\\nlist. We chose to exclude these tasks because our BERT baseline performs better than our human\\nperformance baseline or if the gap between human and machine performance is small.\\nOn Quora Insincere Questions our BERT baseline outperforms our human baseline by a small margin:\\nan F1 score of 67.2 versus 66.7 for BERT and human baselines respectively. Similarly, on the\\nEmpathetic Reactions dataset, BERT outperforms our human baseline, where BERT’s predictions\\nhave a Pearson correlation of 0.45 on empathy and 0.55 on distress, compared to 0.45 and 0.35 for\\nour human baseline. For PAWS-Wiki, we report that BERT achieves an accuracy of 91.9%, while our\\nhuman baseline achieved 84% accuracy. These three tasks are excluded from the benchmark since\\nour, admittedly conservative, human baselines are worse than machine performance. Our human\\nperformance baselines are subject to the clarity of our instructions (all instructions can be found in\\nAppendix C), and crowd workers engagement and ability.\\nFor the Query Well-Formedness task, the authors set an estimate human performance at 88.4%\\naccuracy. Our BERT baseline model reaches an accuracy of 82.3%. While there is a positive gap on\\nthis task, the gap was smaller than we were were willing to tolerate. Similarly, on our recast version\\nof the Ultraﬁne Entity Typing, we observe too small a gap between human (60.2 F1) and machine\\nperformance (55.0 F1). Our recasting for this task is described in Appendix C.2. On GAP, when\\ntaken as a classiﬁcation problem without the related task of span selection (details in C.2), BERT\\nperforms (91.0 F1) comparably to our human baseline (94.9 F1). Given this small margin, we also\\nexclude GAP.\\nOn Discovering Ongoing Conversations, our BERT baseline achieves an F1 of 51.9 on a version of\\nthe task cast as sentence pair classiﬁcation (given two snippets of texts from plays, determine if the\\nsecond snippet is a continuation of the ﬁrst). This dataset is very class imbalanced (90% negative), so\\nwe also experimented with a class-balanced version on which our BERT baselines achieves 88.4\\nF1. Qualitatively, we also found the task challenging for humans as there was little context for the\\ntext snippets and the examples were drawn from plays using early English. Given this fairly high\\nmachine performance and challenging nature for humans, we exclude this task from our benchmark.\\nInstructions tables begin on the following page.\\n13https://www.kaggle.com/c/quora-insincere-questions-classification/data\\n19', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e4cca793-d2e0-4dcc-ac37-6b0c9ff0999d', embedding=None, metadata={'page_label': '20', 'file_name': 'SuperGLUE.pdf', 'file_path': 'data\\\\SuperGLUE.pdf', 'file_type': 'application/pdf', 'file_size': 370981, 'creation_date': '2024-01-30', 'last_modified_date': '2023-12-30', 'last_accessed_date': '2024-01-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 5: The instructions given to crowd-sourced worker describing the training phase for the Choice\\nof Plausible Answers (COPA) task.\\nThe New York University Center for Data Science is collecting your answers for use in research\\non computer understanding of English. Thank you for your help!\\nThis project is a training task that needs to be completed before working on the main project\\non AMT named Human Performance: Plausible Answer. Once you are done with the training,\\nplease proceed to the main task! The qualiﬁcation approval is not immediate but we will add\\nyou to our qualiﬁed workers list within a day.\\nIn this training, you must answer the question on the page and then, to see how you did, click\\ntheCheck Work button at the bottom of the page before hitting Submit. The Check Work\\nbutton will reveal the true label. Please use this training and the provided answers to build\\nan understanding of what the answers to these questions look like (the main project, Human\\nPerformance: Plausible Answer, does not have the answers on the page).\\nTable 6: Task-speciﬁc instructions for Choice of Plausible Alternatives (COPA). These instructions\\nwere provided during both training and annotation phases.\\nPlausible Answer Instructions\\nThe New York University Center for Data Science is collecting your answers for use in research\\non computer understanding of English. Thank you for your help!\\nWe will present you with a prompt sentence and a question. The question will either be about\\nwhat caused the situation described in the prompt, or what a possible effect of that situation is.\\nWe will also give you two possible answers to this question. Your job is to decide, given the\\nsituation described in the prompt, which of the two options is a more plausible answer to the\\nquestion:\\nIn the following example, option 1.is a more plausible answer to the question about what caused\\nthe situation described in the prompt,\\nThe girl received a trophy.\\nWhat’s the CAUSE for this?\\n1.She won a spelling bee.\\n2.She made a new friend.\\nIn the following example, option 2.is a more plausible answer the question about what happened\\nbecause of the situation described in the prompt,\\nThe police aimed their weapons at the fugitive.\\nWhat happened as a RESULT?\\n1.The fugitive fell to the ground.\\n2.The fugitive dropped his gun.\\nIf you have any more questions, please refer to our FAQ page.\\n20', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6aa860ec-ddd7-440c-b1aa-ab96b4d373bd', embedding=None, metadata={'page_label': '21', 'file_name': 'SuperGLUE.pdf', 'file_path': 'data\\\\SuperGLUE.pdf', 'file_type': 'application/pdf', 'file_size': 370981, 'creation_date': '2024-01-30', 'last_modified_date': '2023-12-30', 'last_accessed_date': '2024-01-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 7: Task-speciﬁc instructions for Commitment Bank. These instructions were provided during\\nboth training and annotation phases.\\nSpeaker Commitment Instructions\\nThe New York University Center for Data Science is collecting your answers for use in research\\non computer understanding of English. Thank you for your help!\\nWe will present you with a prompt taken from a piece of dialogue, this could be a single sentence,\\na few sentences, or a short exchange between people. Your job is to ﬁgure out, based on this\\nﬁrst prompt (on top), how certain the speaker is about the truthfulness of the second prompt\\n(on the bottom). You can choose from a 7 point scale ranging from (1) completely certain that\\nthe second prompt is true to (7) completely certain that the second prompt is false. Here are\\nexamples for a few of the labels:\\nChoose 1 (certain that it is true) if the speaker from the ﬁrst prompt deﬁnitely believes or knows\\nthat the second prompt is true. For example,\\n\"What fun to hear Artemis laugh. She’s such a serious child. I didn’t know\\nshe had a sense of humor.\"\\n\"Artemis had a sense of humor\"\\nChoose 4 (not certain if it is true or false) if the speaker from the ﬁrst prompt is uncertain if the\\nsecond prompt is true or false. For example,\\n\"Tess is committed to track. She’s always trained with all her heart and soul.\\nOne can only hope that she has recovered from the ﬂu and will cross the ﬁnish\\nline.\"\\n\"Tess crossed the ﬁnish line.\"\\nChoose 7 (certain that it is false) if the speaker from the ﬁrst prompt deﬁnitely believes or knows\\nthat the second prompt is false. For example,\\n\"Did you hear about Olivia’s chemistry test? She studied really hard. But\\neven after putting in all that time and energy, she didn’t manage to pass the\\ntest\".\\n\"Olivia passed the test.\"\\nIf you have any more questions, please refer to our FAQ page.\\n21', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='eec1a8f9-59e5-4ef6-ba86-0a4ecc714e5a', embedding=None, metadata={'page_label': '22', 'file_name': 'SuperGLUE.pdf', 'file_path': 'data\\\\SuperGLUE.pdf', 'file_type': 'application/pdf', 'file_size': 370981, 'creation_date': '2024-01-30', 'last_modified_date': '2023-12-30', 'last_accessed_date': '2024-01-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 8: Task-speciﬁc instructions for Winograd Schema Challenge (WSC). These instructions were\\nprovided during both training and annotation phases.\\nWinograd Schema Instructions\\nThe New York University Center for Data Science is collecting your answers for use in research\\non computer understanding of English. Thank you for your help!\\nWe will present you with a sentence that someone wrote, with one bolded pronoun. We will then\\nask if you if the pronoun refers to a speciﬁc word or phrase in the sentence. Your job is to ﬁgure\\nout, based on the sentence, if the bolded pronoun refers to this selected word or phrase:\\nChoose Yes if the pronoun refers to the selected word or phrase. For example,\\n\"I put the cake away in the refrigerator. It has a lot of butter in it.\"\\nDoes Itin \"It has a lot\" refer to cake?\\nChoose No if the pronoun does not refer to the selected word or phrase. For example,\\n\"The large ball crashed right through the table because it was made of\\nstyrofoam.\"\\nDoes itin \"it was made\" refer to ball?\\nIf you have any more questions, please refer to our FAQ page.\\n22', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dbc42f7a-9239-4c1e-adfb-60c4990ba917', embedding=None, metadata={'page_label': '23', 'file_name': 'SuperGLUE.pdf', 'file_path': 'data\\\\SuperGLUE.pdf', 'file_type': 'application/pdf', 'file_size': 370981, 'creation_date': '2024-01-30', 'last_modified_date': '2023-12-30', 'last_accessed_date': '2024-01-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 9: Task-speciﬁc instructions for BoolQ (continued in Table 10). These instructions were\\nprovided during both training and annotation phases.\\nQuestion-Answering Instructions\\nThe New York University Center for Data Science is collecting your answers for use in research\\non computer understanding of English. Thank you for your help!\\nWe will present you with a passage taken from a Wikipedia article and a relevant question. Your\\njob is to decide, given the information provided in the passage, if the answer to the question is\\nYes or No. For example,\\nIn the following examples the correct answer is Yes ,\\nThe thirteenth season of Criminal Minds was ordered on April 7, 2017, by\\nCBS with an order of 22 episodes. The season premiered on September 27,\\n2017 in a new time slot at 10:00PM on Wednesday when it had previously\\nbeen at 9:00PM on Wednesday since its inception. The season concluded on\\nApril 18, 2018 with a two-part season ﬁnale.\\nwill there be a 13th season of criminal minds?\\n(In the above example, the ﬁrst line of the passage says that the 13th season of\\nthe show was ordered.)\\nAs of 8 August 2016, the FDA extended its regulatory power to include e-\\ncigarettes. Under this ruling the FDA will evaluate certain issues, including\\ningredients, product features and health risks, as well their appeal to minors\\nand non-users. The FDA rule also bans access to minors. A photo ID is\\nrequired to buy e-cigarettes, and their sale in all-ages vending machines is not\\npermitted. The FDA in September 2016 has sent warning letters for unlawful\\nunderage sales to online retailers and retailers of e-cigarettes.\\nis vaping illegal if you are under 18?\\n(In the above example, the passage states that the \"FDA rule also bans access\\nto minors.\" The question uses the word \"vaping,\" which is a synonym for\\ne-cigrattes.)\\nIn the following examples the correct answer is No ,\\nBadgers are short-legged omnivores in the family Mustelidae, which also\\nincludes the otters, polecats, weasels, and wolverines. They belong to the\\ncaniform suborder of carnivoran mammals. The 11 species of badgers are\\ngrouped in three subfamilies: Melinae (Eurasian badgers), Mellivorinae (the\\nhoney badger or ratel), and Taxideinae (the American badger). The Asiatic\\nstink badgers of the genus Mydaus were formerly included within Melinae\\n(and thus Mustelidae), but recent genetic evidence indicates these are actually\\nmembers of the skunk family, placing them in the taxonomic family Mephitidae.\\nis a wolverine the same as a badger?\\n(In the above example, the passage says that badgers and wolverines are in\\nthe same family, Mustelidae, which does not mean they are the same animal.)\\n23', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e150881d-eb26-471d-af19-c8780bd29d81', embedding=None, metadata={'page_label': '24', 'file_name': 'SuperGLUE.pdf', 'file_path': 'data\\\\SuperGLUE.pdf', 'file_type': 'application/pdf', 'file_size': 370981, 'creation_date': '2024-01-30', 'last_modified_date': '2023-12-30', 'last_accessed_date': '2024-01-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 10: Continuation from Table 9 of task-speciﬁc instructions for BoolQ. These instructions were\\nprovided during both training and annotation phases.\\nMore famously, Harley-Davidson attempted to register as a trademark the\\ndistinctive “chug” of a Harley-Davidson motorcycle engine. On February\\n1, 1994, the company ﬁled its application with the following description:\\n“The mark consists of the exhaust sound of applicant’s motorcycles, produced\\nby V-twin, common crankpin motorcycle engines when the goods are in use. ”\\nNine of Harley-Davidson’s competitors ﬁled oppositions against the applica-\\ntion, arguing that cruiser-style motorcycles of various brands use the same\\ncrankpin V-twin engine which produces the same sound. After six years of\\nlitigation, with no end in sight, in early 2000, Harley-Davidson withdrew their\\napplication.\\ndoes harley davidson have a patent on their sound?\\n(In the above example, the passage states that Harley-Davidson applied for a\\npatent but then withdrew, so they do not have a patent on the sound.)\\nIf you have any more questions, please refer to our FAQ page.\\n24', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9a9f5b28-85d4-4a1b-bb2c-36e8890e72f7', embedding=None, metadata={'page_label': '25', 'file_name': 'SuperGLUE.pdf', 'file_path': 'data\\\\SuperGLUE.pdf', 'file_type': 'application/pdf', 'file_size': 370981, 'creation_date': '2024-01-30', 'last_modified_date': '2023-12-30', 'last_accessed_date': '2024-01-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 11: Task-speciﬁc instructions for the diagnostic and the bias diagnostic datasets. These\\ninstructions were provided during both training and annotation phases.\\nTextual Entailment Instructions\\nThe New York University Center for Data Science is collecting your answers for use in research\\non computer understanding of English. Thank you for your help!\\nWe will present you with a prompt taken from an article someone wrote. Your job is to ﬁgure out,\\nbased on this correct prompt (the ﬁrst prompt, on top), if another prompt (the second prompt, on\\nbottom) is also necessarily true:\\nChoose True if the event or situation described by the ﬁrst prompt deﬁnitely implies that the\\nsecond prompt, on bottom, must also be true. For example,\\n•\"Murphy recently decided to move to London.\"\\n\"Murphy recently decided to move to England.\"\\n(The above example is True because London is in England and therefore prompt 2 is\\nclearly implied by prompt 1.)\\n•\"Russian cosmonaut Valery Polyakov set the record for the longest continuous amount\\nof time spent in space, a staggering 438 days, between 1994 and 1995.\"\\n\"Russians hold record for longest stay in space.\"\\n(The above example is True because the information in the second prompt is contained\\nin the ﬁrst prompt: Valery is Russian and she set the record for longest stay in space.)\\n•\"She does not disgree with her brother’s opinion, but she believes he’s too aggresive in\\nhis defense\"\\n\"She agrees with her brother’s opinion, but she believes he’s too aggresive in his\\ndefense\"\\n(The above example is True because the second prompt is an exact paraphrase of the\\nﬁrst prompt, with exactly the same meaning.)\\nChoose False if the event or situation described with the ﬁrst prompt on top does not necessarily\\nimply that this second prompt must also be true. For example,\\n•\"This method was developed at Columbia and applied to data processing at CERN.\"\\n\"This method was developed at Columbia and applied to data processing at CERN\\nwith limited success.\"\\n(The above example is False because the second prompt is introducing new information\\nnot implied in the ﬁrst prompt: The ﬁrst prompt does not give us any knowledge of\\nhow succesful the application of the method at CERN was.)\\n•\"This building is very tall.\"\\n\"This is the tallest building in New York.\"\\n(The above example is False because a building being tall does not mean it must be the\\ntallest building, nor that it is in New York.)\\n•\"Hours earlier, Yasser Arafat called for an end to attacks against Israeli civilians in\\nthe two weeks before Israeli elections.\"\\n\"Arafat condemned suicide bomb attacks inside Israel.\"\\n(The above example is False because from the ﬁrst prompt we only know that Arafat\\ncalled for an end to attacks against Israeli citizens, we do not know what kind of attacks\\nhe may have been condemning.)\\nYou do not have to worry about whether the writing style is maintained between the two prompts.\\nIf you have any more questions, please refer to our FAQ page.\\n25', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='22e34f61-51fa-4717-bac4-dd6cd4cb30a8', embedding=None, metadata={'page_label': '26', 'file_name': 'SuperGLUE.pdf', 'file_path': 'data\\\\SuperGLUE.pdf', 'file_type': 'application/pdf', 'file_size': 370981, 'creation_date': '2024-01-30', 'last_modified_date': '2023-12-30', 'last_accessed_date': '2024-01-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 12: Task-speciﬁc instructions for the Gendered Ambiguous Pronoun Coreference (GAP) task.\\nThese instructions were provided during both training and annotation phases.\\nGAP Instructions\\nThe New York University Center for Data Science is collecting your answers for use in research\\non computer understanding of English. Thank you for your help!\\nWe will present you with an extract from a Wikipedia article, with one bolded pronoun. We will\\nalso give you two names from the text that this pronoun could refer to. Your job is to ﬁgure out,\\nbased on the extract, if the pronoun refers to option A, options B, or neither:\\nChoose A if the pronoun refers to option A. For example,\\n\"In 2010 Ella Kabambe was not the ofﬁcial Miss Malawi; this was Faith\\nChibale, but Kabambe represented the country in the Miss World pageant.\\nAt the 2012 Miss World, Susan Mtegha pushed Miss New Zealand, Collette\\nLochore, during the opening headshot of the pageant, claiming that Miss New\\nZealand was in her space.\"\\nDoes herrefer to option A or B below?\\nASusan Mtegha\\nBCollette Lochore\\nCNeither\\nChoose B if the pronoun refers to option B. For example,\\n\"In 1650 he started his career as advisor in the ministerium of ﬁnances in Den\\nHaag. After he became a minister he went back to Amsterdam, and took place\\nas a sort of chairing mayor of this city. After the death of his brother Cornelis,\\nDe Graeff became the strong leader of the republicans. He held this position\\nuntil the rampjaar.\"\\nDoes Herefer to option A or B below?\\nACornelis\\nBDe Graeff\\nCNeither\\nChoose C if the pronoun refers to neither option. For example,\\n\"Reb Chaim Yaakov’s wife is the sister of Rabbi Moishe Sternbuch, as is\\nthe wife of Rabbi Meshulam Dovid Soloveitchik, making the two Rabbis his\\nuncles. Reb Asher’s brother Rabbi Shlomo Arieli is the author of a critical\\nedition of the novallae of Rabbi Akiva Eiger. Before his marriage, Rabbi Arieli\\nstudied in the Ponevezh Yeshiva headed by Rabbi Shmuel Rozovsky, and he\\nlater studied under his father-in-law in the Mirrer Yeshiva.\"\\nDoes hisrefer to option A or B below?\\nAReb Asher\\nBAkiva Eiger\\nCNeither\\nIf you have any more questions, please refer to our FAQ page.\\n26', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0870b621-1ff2-4f08-b1c4-6bc15088c0c1', embedding=None, metadata={'page_label': '27', 'file_name': 'SuperGLUE.pdf', 'file_path': 'data\\\\SuperGLUE.pdf', 'file_type': 'application/pdf', 'file_size': 370981, 'creation_date': '2024-01-30', 'last_modified_date': '2023-12-30', 'last_accessed_date': '2024-01-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 13: Task-speciﬁc instructions for the Paraphrase Adversaries from Word Scrambling (PAWS)\\ntask. These instructions were provided during both training and annotation phases.\\nParaphrase Detection Instructions\\nThe New York University Center for Data Science is collecting your answers for use in research\\non computer understanding of English. Thank you for your help!\\nWe will present you with two similar sentences taken from Wikipedia articles. Your job is to\\nﬁgure out if these two sentences are paraphrases of each other, and convey exactly the same\\nmeaning:\\nChoose Yes if the sentences are paraphrases and have the exact same meaning. For example,\\n\"Hastings Ndlovu was buried with Hector Pieterson at Avalon Cemetery in\\nJohannesburg.\"\\n\"Hastings Ndlovu , together with Hector Pieterson , was buried at the Avalon\\ncemetery in Johannesburg .\"\\n\"The complex of the Trabzon World Trade Center is close to Trabzon Airport\\n.\"\\n\"The complex of World Trade Center Trabzon is situated close to Trabzon\\nAirport .\"\\nChoose Noif the two sentences are not exact paraphrases and mean different things. For\\nexample,\\n\"She was only a few months in French service when she met some British\\nfrigates in 1809 .\"\\n\"She was only in British service for a few months , when in 1809 , she\\nencountered some French frigates .\"\\n\"This work caused him to trigger important reﬂections on the practices of\\nmolecular genetics and genomics at a time when this was not considered\\nethical .\"\\n\"This work led him to trigger ethical reﬂections on the practices of molecular\\ngenetics and genomics at a time when this was not considered important .\"\\nIf you have any more questions, please refer to our FAQ page.\\n27', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fdab4875-9b13-40f7-972c-715cf15aabae', embedding=None, metadata={'page_label': '28', 'file_name': 'SuperGLUE.pdf', 'file_path': 'data\\\\SuperGLUE.pdf', 'file_type': 'application/pdf', 'file_size': 370981, 'creation_date': '2024-01-30', 'last_modified_date': '2023-12-30', 'last_accessed_date': '2024-01-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 14: Task-speciﬁc instructions for the Quora Insincere Questions task. These instructions were\\nprovided during both training and annotation phases.\\nInsincere Questions Instructions\\nThe New York University Center for Data Science is collecting your answers for use in research\\non computer understanding of English. Thank you for your help!\\nWe will present you with a question that someone posted on Quora. Your job is to ﬁgure out\\nwhether or not this is a sincere question. An insincere question is deﬁned as a question intended\\nto make a statement rather than look for helpful answers. Some characteristics that can signify\\nthat a question is insincere:\\n•Has a non-neutral tone\\n–Has an exaggerated tone to underscore a point about a group of people\\n–Is rhetorical and meant to imply a statement about a group of people\\n•Is disparaging or inﬂammatory\\n–Suggests a discriminatory idea against a protected class of people, or seeks\\nconﬁrmation of a stereotype\\n–Makes disparaging attacks/insults against a speciﬁc person or group of people\\n–Based on an outlandish premise about a group of people\\n–Disparages against a characteristic that is not ﬁxable and not measurable\\n•Isn’t grounded in reality\\n–Based on false information, or contains absurd assumptions\\n–Uses sexual content (incest, bestiality, pedophilia) for shock value, and not to seek\\ngenuine answers\\nPlease note that there are far fewer insincere questions than there are sincere questions! So you\\nshould expect to label most questions as sincere.\\nExamples,\\nChoose Sincere if you believe the person asking the question was genuinely seeking an answer\\nfrom the forum. For example,\\n\"How do DNA and RNA compare and contrast?\"\\n\"Are there any sports that you don’t like? \"\\n\"What is the main purpose of penance?\"\\nChoose Insincere if you believe the person asking the question was not really seeking an answer\\nbut was being inﬂammatory, extremely rhetorical, or absurd. For example,\\n\"How do I sell Pakistan? I need lots of money so I decided to sell Pakistan\\nany one wanna buy?\"\\n\"If Hispanics are so proud of their countries, why do they move out?\"\\n\"Why Chinese people are always not welcome in all countries?\"\\nIf you have any more questions, please refer to our FAQ page.\\n28', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e57af2d2-c204-4600-8074-8f162f23ce26', embedding=None, metadata={'page_label': '29', 'file_name': 'SuperGLUE.pdf', 'file_path': 'data\\\\SuperGLUE.pdf', 'file_type': 'application/pdf', 'file_size': 370981, 'creation_date': '2024-01-30', 'last_modified_date': '2023-12-30', 'last_accessed_date': '2024-01-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 15: Task-speciﬁc instructions for the Ultraﬁne Entity Typing task. These instructions were\\nprovided during both training and annotation phases.\\nEntity Typing Instructions\\nThe New York University Center for Data Science is collecting your answers for use in research\\non computer understanding of English. Thank you for your help!\\nWe will provide you with a sentence with on bolded word or phrase. We will also give you a\\npossible tag for this bolded word or phrase. Your job is to decide, in the context of the sentence,\\nif this tag is correct and applicable to the bolded word or phrase:\\nChoose Yesif the tag is applicable and accurately describes the selected word or phrase. For\\nexample,\\n“Spain was the gold line.\" Itstarted out with zero gold in 1937, and by 1945\\nit had 65.5 tons.\\nTag: nation\\nChoose Noif the tag is not applicable and does not describes the selected word or phrase. For\\nexample,\\nIraqi museum workers are starting to assess the damage to Iraq’s history.\\nTag: organism\\nIf you have any more questions, please refer to our FAQ page.\\n29', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1fddf266-da8c-4cc2-b830-4ad84f2e1db8', embedding=None, metadata={'page_label': '30', 'file_name': 'SuperGLUE.pdf', 'file_path': 'data\\\\SuperGLUE.pdf', 'file_type': 'application/pdf', 'file_size': 370981, 'creation_date': '2024-01-30', 'last_modified_date': '2023-12-30', 'last_accessed_date': '2024-01-30'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 16: Task-speciﬁc instructions for the Empathetic Reaction task. These instructions were\\nprovided during both training and annotation phases.\\nEmpathy and Distress Analysis Instructions\\nThe New York University Center for Data Science is collecting your answers for use in research\\non computer understanding of English. Thank you for your help!\\nWe will present you with a message someone wrote after reading an article. Your job is to ﬁgure\\nout, based on this message, how disressed and empathetic the author was feeling. Empathy is\\ndeﬁned as feeling warm, tender, sympathetic, moved, or compassionate. Distressed is deﬁned as\\nfeeling worried, upset, troubled, perturbed, grieved, distrubed, or alarmed.\\nExamples,\\nThe author of the following message was not feeling empathetic at all with an empathy score of 1 ,\\nand was very distressed with a distress score of 7 ,\\n\"I really hate ISIS. They continue to be the stain on society by committing\\natrocities condemned by every nation in the world. They must be stopped at\\nall costs and they must be destroyed so that they wont hurt another soul. These\\npoor people who are trying to survive get killed, imprisoned, or brainwashed\\ninto joining and there seems to be no way to stop them.\"\\nThe author of the following message is feeling very empathetic with an empathy score of 7 and\\nalso very distressed with a distress score of 7 ,\\n\"All of you know that I love birds. This article was hard for me to read because\\nof that. Wind turbines are killing a lot of birds, including eagles. It’s really\\nvery sad. It makes me feel awful. I am all for wind turbines and renewable\\nsources of energy because of global warming and coal, but this is awful. I\\ndon’t want these poor birds to die like this. Read this article and you’ll see\\nwhy.\"\\nThe author of the following message is feeling moderately empathetic with an\\nempathy score of 4 and moderately distressed with a distress score of 4 ,\\n\"I just read an article about wild ﬁres sending a smokey haze across the state\\nnear the Appalachian mountains. Can you imagine how big the ﬁre must be\\nto spread so far and wide? And the people in the area obviously suffer the\\nmost. What if you have asthma or some other condition that restricts your\\nbreathing?\"\\nThe author of the following message is feeling very empathetic with an empathy score of 7 and\\nmildly distressed with a distress score of 2 ,\\n\"This is a very sad article. Being of of the ﬁrst female ﬁghter pilots must\\nhave given her and her family great honor. I think that there should be more\\ntraining for all pilots who deal in these acrobatic ﬂying routines. I also think\\nthat women have just as much of a right to become a ﬁghter pilot as men.\"\\nIf you have any more questions, please refer to our FAQ page.\\n30', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\Python Data sceience\\Projects\\GenAI\\RAG_lama_index\\venvs\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Parsing nodes: 100%|██████████| 38/38 [00:00<00:00, 50.03it/s]\n",
      "Generating embeddings: 100%|██████████| 61/61 [00:06<00:00,  9.15it/s]\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.indices.vector_store.base.VectorStoreIndex at 0x2924fa45d80>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine at 0x2924fa47d60>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What is ROUGE ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It is a package used for the automatic evaluation of summaries. ROUGE includes measures that compare a computer-generated summary to ideal summaries created by humans to determine the quality of the summary. These measures count the number of overlapping units such as n-grams, word sequences, and word pairs. ROUGE introduces four different measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S. These measures have been used in large-scale summarization evaluations sponsored by NIST.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What is SuperGLUE ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SuperGLUE is a benchmark that is designed to evaluate the performance of language understanding systems. It consists of a set of difficult language understanding tasks, a software toolkit, and a public leaderboard. The goal of SuperGLUE is to provide a more challenging benchmark compared to the previous GLUE benchmark, which has been surpassed by the performance of non-expert humans. SuperGLUE aims to test a system's ability to understand and reason about English texts, and the tasks included in the benchmark are designed to be solvable by most college-educated English speakers without requiring domain-specific knowledge.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: SuperGLUE is a benchmark that is designed to evaluate\n",
      "the performance of language understanding systems. It consists of a\n",
      "set of difficult language understanding tasks, a software toolkit, and\n",
      "a public leaderboard. The goal of SuperGLUE is to provide a more\n",
      "challenging benchmark compared to the previous GLUE benchmark, which\n",
      "has been surpassed by the performance of non-expert humans. SuperGLUE\n",
      "aims to test a system's ability to understand and reason about English\n",
      "texts, and the tasks included in the benchmark are designed to be\n",
      "solvable by most college-educated English speakers without requiring\n",
      "domain-specific knowledge.\n",
      "______________________________________________________________________\n",
      "Source Node 1/2\n",
      "Node ID: 16621d78-75f8-4aa4-b9f1-5579d68d7bad\n",
      "Similarity: 0.8214322531137586\n",
      "Text: SuperGLUE: A Stickier Benchmark for General-Purpose Language\n",
      "Understanding Systems Alex Wang∗ New York UniversityYada\n",
      "Pruksachatkun∗ New York UniversityNikita Nangia∗ New York University\n",
      "Amanpreet Singh∗ Facebook AI ResearchJulian Michael University of\n",
      "WashingtonFelix Hill DeepMindOmer Levy Facebook AI Research Samuel R.\n",
      "Bowman New York Universi...\n",
      "______________________________________________________________________\n",
      "Source Node 2/2\n",
      "Node ID: 7e99996c-9842-421a-8622-257faae2577d\n",
      "Similarity: 0.8111711139567556\n",
      "Text: While some initially difﬁcult categories saw gains from advances\n",
      "on GLUE (e.g., double negation), others remain hard (restrictivity) or\n",
      "even adversarial (disjunction, downward monotonicity). This suggests\n",
      "that even as unsupervised pretraining produces ever-better statistical\n",
      "summaries of text, it remains difﬁcult to extract many details crucial\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "from llama_index.response.pprint_utils import pprint_response\n",
    "pprint_response(response, show_source=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.indices.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "retriever = VectorIndexRetriever(index= index, similarity_top_k= 3)\n",
    "\n",
    "query_engine = RetrieverQueryEngine(retriever= retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What is SuperGLUE ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: SuperGLUE is a new benchmark that is designed to\n",
      "provide a more rigorous test of language understanding. It consists of\n",
      "a public leaderboard with eight language understanding tasks,\n",
      "including tasks like coreference resolution and question answering.\n",
      "SuperGLUE aims to pose more challenging tasks that go beyond the scope\n",
      "of current state-of-the-art systems, but are solvable by most college-\n",
      "educated English speakers. It also includes comprehensive human\n",
      "baselines for all benchmark tasks and comes with an improved code\n",
      "support toolkit for pretraining, multi-task learning, and transfer\n",
      "learning in NLP.\n",
      "______________________________________________________________________\n",
      "Source Node 1/3\n",
      "Node ID: 16621d78-75f8-4aa4-b9f1-5579d68d7bad\n",
      "Similarity: 0.8214322531137586\n",
      "Text: SuperGLUE: A Stickier Benchmark for General-Purpose Language\n",
      "Understanding Systems Alex Wang∗ New York UniversityYada\n",
      "Pruksachatkun∗ New York UniversityNikita Nangia∗ New York University\n",
      "Amanpreet Singh∗ Facebook AI ResearchJulian Michael University of\n",
      "WashingtonFelix Hill DeepMindOmer Levy Facebook AI Research Samuel R.\n",
      "Bowman New York Universi...\n",
      "______________________________________________________________________\n",
      "Source Node 2/3\n",
      "Node ID: 7e99996c-9842-421a-8622-257faae2577d\n",
      "Similarity: 0.8111711139567556\n",
      "Text: While some initially difﬁcult categories saw gains from advances\n",
      "on GLUE (e.g., double negation), others remain hard (restrictivity) or\n",
      "even adversarial (disjunction, downward monotonicity). This suggests\n",
      "that even as unsupervised pretraining produces ever-better statistical\n",
      "summaries of text, it remains difﬁcult to extract many details crucial\n",
      "...\n",
      "______________________________________________________________________\n",
      "Source Node 3/3\n",
      "Node ID: a673e2d9-4e14-4cee-9bd4-110d6b8e8827\n",
      "Similarity: 0.8045732840023417\n",
      "Text: BiLSTM+ELMo+Attn OpenAI GPT BERT + Single-task Adapters BERT\n",
      "(Large) BERT on STILTs BERT + BAM SemBERT Snorkel MeTaL ALICE (Large)\n",
      "MT-DNN (ensemble) XLNet-Large (ensemble)0.50.60.70.80.91.01.11.2 GLUE\n",
      "Score Human Performance CoLA SST-2MRPC STS-B QQP MNLIQNLI RTE\n",
      "WNLIFigure 1: GLUE benchmark performance for submitted systems,\n",
      "rescaled to set huma...\n",
      "SuperGLUE is a new benchmark that is designed to provide a more rigorous test of language understanding. It consists of a public leaderboard with eight language understanding tasks, including tasks like coreference resolution and question answering. SuperGLUE aims to pose more challenging tasks that go beyond the scope of current state-of-the-art systems, but are solvable by most college-educated English speakers. It also includes comprehensive human baselines for all benchmark tasks and comes with an improved code support toolkit for pretraining, multi-task learning, and transfer learning in NLP.\n"
     ]
    }
   ],
   "source": [
    "pprint_response(response, show_source=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocessor = SimilarityPostprocessor(similarity_cutoff= 0.80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = RetrieverQueryEngine(retriever= retriever, node_postprocessors= [postprocessor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What is use of SuperGLUE ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: SuperGLUE is a benchmark that is designed to provide a\n",
      "simple and robust evaluation metric for any method capable of being\n",
      "applied to a broad range of language understanding tasks. It offers a\n",
      "new set of more difficult language understanding tasks, along with a\n",
      "software toolkit and a public leaderboard. The goal of SuperGLUE is to\n",
      "test a system's ability to understand and reason about texts written\n",
      "in English, while also being beyond the scope of current state-of-the-\n",
      "art systems. It excludes tasks that require domain-specific knowledge\n",
      "and aims to be solvable by most college-educated English speakers. The\n",
      "SuperGLUE benchmark can be used to evaluate custom models and training\n",
      "methods on the benchmark tasks, and it supports popular pretrained\n",
      "models such as OpenAI GPT and BERT.\n",
      "______________________________________________________________________\n",
      "Source Node 1/3\n",
      "Node ID: 16621d78-75f8-4aa4-b9f1-5579d68d7bad\n",
      "Similarity: 0.8123687903359365\n",
      "Text: SuperGLUE: A Stickier Benchmark for General-Purpose Language\n",
      "Understanding Systems Alex Wang∗ New York UniversityYada\n",
      "Pruksachatkun∗ New York UniversityNikita Nangia∗ New York University\n",
      "Amanpreet Singh∗ Facebook AI ResearchJulian Michael University of\n",
      "WashingtonFelix Hill DeepMindOmer Levy Facebook AI Research Samuel R.\n",
      "Bowman New York Universi...\n",
      "______________________________________________________________________\n",
      "Source Node 2/3\n",
      "Node ID: 7e99996c-9842-421a-8622-257faae2577d\n",
      "Similarity: 0.8112404415080156\n",
      "Text: While some initially difﬁcult categories saw gains from advances\n",
      "on GLUE (e.g., double negation), others remain hard (restrictivity) or\n",
      "even adversarial (disjunction, downward monotonicity). This suggests\n",
      "that even as unsupervised pretraining produces ever-better statistical\n",
      "summaries of text, it remains difﬁcult to extract many details crucial\n",
      "...\n",
      "______________________________________________________________________\n",
      "Source Node 3/3\n",
      "Node ID: f38d975a-4895-435e-959f-ea1b3df50848\n",
      "Similarity: 0.804665730300527\n",
      "Text: 4 Using SuperGLUE Software Tools To facilitate using SuperGLUE,\n",
      "we release jiant (Wang et al., 2019b),7a modular software toolkit,\n",
      "built with PyTorch (Paszke et al., 2017), components from AllenNLP\n",
      "(Gardner et al., 2017), and the pytorch-pretrained-bert package.8jiant\n",
      "implements our baselines and supports the evaluation of custom models\n",
      "and trai...\n",
      "SuperGLUE is a benchmark that is designed to provide a simple and robust evaluation metric for any method capable of being applied to a broad range of language understanding tasks. It offers a new set of more difficult language understanding tasks, along with a software toolkit and a public leaderboard. The goal of SuperGLUE is to test a system's ability to understand and reason about texts written in English, while also being beyond the scope of current state-of-the-art systems. It excludes tasks that require domain-specific knowledge and aims to be solvable by most college-educated English speakers. The SuperGLUE benchmark can be used to evaluate custom models and training methods on the benchmark tasks, and it supports popular pretrained models such as OpenAI GPT and BERT.\n"
     ]
    }
   ],
   "source": [
    "pprint_response(response, show_source=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from llama_index import (VectorStoreIndex, SimpleDirectoryReader,\n",
    "                         StorageContext, load_index_from_storage,\n",
    "                         )\n",
    "\n",
    "# check if storage already exists\n",
    "PERSIT_DIR = \"./storage\"\n",
    "if not os.path.exists(PERSIT_DIR):\n",
    "    # load the documents and create the index\n",
    "    documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    # store it for later\n",
    "    index.storage_context.persist(persist_dir= PERSIT_DIR)\n",
    "else:\n",
    "    # load the existing index\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIT_DIR)\n",
    "    index = load_index_from_storage(storage_context)\n",
    "\n",
    "# query the index\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is SuperGLUE and its use ?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
